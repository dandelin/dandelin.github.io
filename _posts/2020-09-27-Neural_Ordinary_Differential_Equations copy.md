---
layout: post
title: "Neural Ordinary Differential Equations"
published: false
# date: 2020-09-27
# comments: true
---

### Good Old Neural Nets
다양한 파라미터화된 함수들의 조합을 우리는 뉴럴 네트워크라고 부릅니다 : $u \mapsto f_S \circ \cdots f_0(u) := \phi_S(u)$.
그런데 여기서 함수들의 갯수가 $S$개가 아니라 무한개라면 어떻게 될까요?
함수가 무한개여도 어떤 함수를 먼저 적용할지는 순서가 정해져있으며, 이러한 성질은 실수체 $\mathbb{R}$의 성질과 매우 흡사합니다.
우리가 미분을 통해서 실함수를 선형화 하듯, $\phi$ 속 함수의 나열을 선형화된 데이터 맵핑의 부분으로 볼 수 있지 않을까요?

맵핑의 부분들을 모아서 맵핑을 구현하기 위해서는 두가지 장치가 필요합니다.
하나는 `부분`의 정의인데, 상미분방정식으로 일단 정의해봅시다: $\frac{dx}{ds} = f(s, x(s))$.
또 다른 하나는 `부분의 접합` $\int$인데, 다양한 구현을 해 볼 수 있습니다: 가장 간단하게는 `rk4`와 같은 수치해석적 솔버를 쓸 수 있겠죠.
그러면 위의 무한개 함수의 컴포짓 $\lim_{S \to \inf} \phi_S(u)$를 $\int_{0}^{S} f(s, x(s)) ds$로 나타낼 수 있습니다.
데이터 함수 (혹은, state) $x(s)$는 `순서` $s$에서의 시작점 $u$에서 데이터가 어떻게 변화되었냐를 의미하고, $x(0)=u$이며 $x(S)=\phi_S(u)$입니다.
이 함수 $f$를 다시 우리가 잘 아는 뉴럴넷으로 모델링 할 수 있고, 이 경우 $f$를 Neural Ordinary Differential Equations (NODE)라고 칭합니다.

뭔가 이상하죠, 네, 뉴럴넷 $u \mapsto f_S \circ \cdots f_0(u) := \phi_S(u)$에서는 각 함수 $f_0, f_1, f_2, \dots$가 다 다른 함수이지만, 위 극한에서는 함수 $f$로 하나로 퉁칩니다.
그래서 더 적합한 표현을 위해서는 $f$ 또한 순서 $s$에 의해서 변해야 할 필요가 있습니다.
다양한 방법을 통해서 구현할 수 있는데, 그나마 가장 간단한 방법은 순서 $s$를 입력으로 받아서 $f$의 파라미터를 출력하는 파라미터함수 $w$를 정의해서 $f := f(s, x(s), w(s))$와 같이 $f$를 다시 정의하는 겁니다.

모델링 하고자 하는 것을 바로 함수로 표현하지 않고 대신 부분들을 표현한 다음 적분을 통해 모델을 구하는 이러한 방법은 `접평면`을 통한 우리 우주, 비 유클리드 공간의 모델링처럼 바로 함수로 표현하는 것 보다 훨씬 더 넓은 범주를 비슷한 자원으로 다룰 수 있게 해줍니다.
즉, 최적화 입장에서는 훨씬 더 많은 값들을 비교해봐야 한다는 의미죠.
기존 뉴럴넷을 학습할때의 옵티마이저와 학습 플랜을 그대로 사용한다면, 그 최적화가 보장하는 "내가 본" 모델링의 영역이 훨씬 작아지게 되고, 이는 곧 민감[^1]한 모델을 야기하게 됩니다.
민감한 모델은 수치해석적 솔버가 적분값을 구하는데 더 많은 이벨류에이션을 해야할 뿐 만 아니라, 애드버서리얼 어택을 더 잘 허용하는 덜 강인한 모델이랑 동의어죠.

공짜 점심은 없습니다. 우리가 현 시대에서 가용가능한 옵티마이저를 사용하길 원한다면, $f$가 표현할 수 있는 범위는 다시 우리의 친숙한 수백개 레이어의 뉴럴넷 $\phi$와 동등해질 겁니다.
그러면 자유도가 제약된 NODE를 왜 기존 뉴럴넷 대신 사용하려고 하는걸까요?
그건 물론, 그 두개가 완전히 같은 함수공간을 탐색하지 않기 때문이죠.
뭐가 뭐에 더 좋은지는 앞으로 있을 많은 연구들이 밝혀줄 테니까, 일단은 $f$를 어떻게 강인하게 만들지부터 봅시다.

### Enforcing Stability

#### Neural ODE
$\dot{x} = f(u, x(s), w(s))$; $x(0)=u$; $\hat{y}=x(S)$; $s \in [0, S]$. 여기서 Neural ODE란 $f$, 더 광의적으로는 $f$가 기술하는 시스템을 의미합니다.

#### Well-Posedness
$f$가 립슈이츠, 즉 $d_Y(f(x), f(x')) \le Kd_X(x, x')$라면, 모든 시작점 $u$에 대해서 상태 (솔루션) $x$는 유일하게 정해집니다.

#### Stable Neural Flow
스테이블 뉴럴 플로우 $\epsilon$은 다음을 만족하는 NODE입니다: $\dot{x}=-\partial_{x} \epsilon(u, x(s), w(s))$.
랴푸노프 안정성 테스트를 해봅시다.
$(\partial_{x} \epsilon)^2$은 $\mathbb{R}^{n_x}$ 에서 항상 양수이므로, 랴푸노프 대상 $V(x)$로 쓰일 수 있습니다.
$\dot{V}(x) = V'(x) \dot{x} = 2(\partial_{x} \epsilon) * (-\partial_{x} \epsilon) = -2 (\partial_{x} \epsilon)^2 \le 0$이며, 만약 어떤 집합 $M \subset \mathbb{R}^{n_x}$ 속 $x \in M$이 항상 $\partial_{x} \epsilon = \mathbb{0}_{n_x}$라면, 이 $M$은 랴푸노프 스테이블 포인트로 작용합니다.
즉 $\lim{s \to \inf}$일 때 $\phi_s (w, u) \in M$입니다.

#### Energy
머신러닝 컨텍스트에서 에너지라는 말이 쓰인다면, 그건 어떤 양<sub>quantity</sub>이며, 그 값의 보존이 명시적으로 드러나있는 경우입니다.
물리학의 에너지 보존 법칙에서 따온 말이죠.
스테이블 뉴럴 플로우 $\epsilon$은 이 에너지의 앞 글자를 따왔습니다.

#### Port-Hamiltonian Model
행렬 $A(x)$를 계수로 사용하여 $\dot{x} = A(x)\partial_{x} \epsilon(u, x, w)$를 정의하고, $A(x) + A^T(x) ≺ 0$ ($≺$는 좌항의 모든 인자가 우항보다 작음을 의미합니다.)라면, 이는 스테이블 뉴럴 플로우에 조금 더 다양성을 부여해 줄 수 있겠지요.
$A(x)$를 구조행렬<sub>structure matrix</sub>로 보면, $x$를 포트-해밀토니언 시스템으로 볼 수 있습니다.

#### Second-Order Model
$x$를 둘로 쪼개서 볼까요? $x := (q, p)$; $p, q \in \mathbb{R}^{n_v}$; $n_v = \frac{n_x}{2}$.
그럼 다음 $p$와 $q$에 대한 시스템을 정의합니다.
(1) $\dot{q} = p$, (2) $\dot{p} = -\alpha p - \partial_{q} \epsilon(u, q(s), w(s))$.
이렇게 되면 토탈 에너지 함수 $\phi(q, p) = \frac{1}{2} p^T p + \epsilon(q, w)$를 정의하고,






























[^1]: 인풋이 조금 변해도 아웃풋이 크게 변하는 경우를 뜻합니다.