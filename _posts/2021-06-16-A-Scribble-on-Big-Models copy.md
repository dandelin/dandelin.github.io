---
layout: post
title: "A Scribble on Big Models"
date: 2021-06-16
comments: true
published: false
---

모델의 *scaling law*는 21년 중반에 들어선 지금, 내 생각으로는 무엇보다 딥러닝이 왜 잘되는가에 대한 진리를 전달해준다고 믿는다.
또한 모델이 인풋으로 데이터 포인트 x를 받아 또 다른 포인트 y를 내놓는 도식화는, 지금의 딥러닝 모델들과 잘 맞지 않는 설명이다.
그보다는 모델은 인풋으로 어떤 *집합*을 받고 그 집합간의 *관계*에 대한 다른 집합을 내놓는다고 도식화해야 훨씬 직관적으로 맞는 것 같다.
데이터 포인트 x가 벡터

인지이론중 하나의 가설은 인간은 어떠한 개념을 저장할때 그 개념을 트리의 노드로 저장한다고 한다.
물론, 개념은 "비행기" 같은 단순한 단어가 아니라 (그렇게 설명하면 더 편하기는 하겠지만), 신경망이 점화해 표기하는 아날로그 신호로 나타날 것이다.
개념은 동시에 여러 서브트리에 속할 수 있고, 이 트리들의 토폴로지는 개념군을 비교하여 인간의 유비추론을 돕는다.

신경 표상을 나타내는 해상도는 인간의 뉴런의 수에 비례한 어떤 값일 것이고, 그 재료인 뇌 속 뉴런의 개수는 86B개로 GPT-3의 "뉴런"의 개수의 절반 정도이다.
단어는 같지만 두 연산이 가지는 특징중 비슷한 것을 꼽기가 더 어렵다.

1. 뇌는 아날로그 연산이며 GPT-3는 디지털 연산이다.
2. 뇌는 인지 시간동안 정보 전달을 진행하며, 한 뉴런은 해당 시간의 정보 전달에 연속적이고 지속적으로 관여한다. / GPT-3는 정해진 레이어의 개수만큼 정보 전달을 진행하며 한 뉴런은 관여한 레이어 외에는 사용되지 않는다.
3. 뉴런들의 연결도는 시간에 따라 연속적으로, 그리고 자연히 마르코프적으로 변하며, 표면상으로 드러난 어떠한 목표 함수에 대한 최적화 이론을 따르지 않는다. / GPT-3의 DAG구조를 따라 목표 함수의 아웃풋의 그래디언트로 연결도를 업데이트한다.

하지만 개념적으로 드러나는 공통점이 있는데, 바로 인풋으로 받은 집합의 원소들의 연결관계를 모델링한다는 것이다.
이 모델링은 들어온 인풋 집합의 원소들의 고수준 연결관계(받은 집합이 {1, 2, 3, 4, 5}였으면 {1, 1, 3, 4, 2, 5, ..., 8} 등 높은 차수의 관계)를 넘어서서 모델이 파라미터로서 들고있는 것도 연결관계를 구할 때는 인풋으로 사용해야 한다.
즉 인간의 뇌라면 인지 뉴런을 {a, b, c, ...}로 표기하고 감각 수용 뉴런을 {1, 2, 3, ...}으로 표기할 때 {b, 2, 4, 5, v, ..., z, a}와 같은 관계를 모델링 해야하며,
GPT-3라면 모델 웨이트들을 {a, b, c, ...}로 표기하고 인풋 텍스트 토큰들을 {1, 2, 3, ...}으로 표기할 때 마찬가지로 {b, 2, 4, 5, v, ..., z, a}와 같은 관계를 모델링 해야한다.

GPT-3에서 파라미터의 개수는 가소성으로 변할 수 있는 인지 표현의 공간적 여유를 제공하며, 레이어의 개수는 표현할 수 있는 관계의 차수를 의미한다.
파라미터의 개수와 레이어의 개수는 scaling law의 주요 독립변인으로, 적절한 비율을 찾으면 적절한 인풋이 스케일 아웃할 때 그 비율을 따라 스케일 아웃함으로써 데이터에 녹아있는 *개념*들을 관계 트리의 노드들로 *저장*할 수 있다.
이러면 충분히 높은 차수의 연결관계를 충분히 많은 개념에 대해서 구축할 수 있는데 *오버피팅*이 일어난다는게 오히려 비직관적이 된다.
파라미터가 많을수록 오버피팅이 일어난다는 얘기는 고수준 연결관계를 구축할 연산 그래프가 정의되지 않은 상태이거나 개념을 추출할 데이터가 파라미터의 개수와 레이어의 개수보다 턱없이 적을때나 일어날 수 있는 현상이 되는 것이다.