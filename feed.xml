<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://wonjae.kim/feed.xml" rel="self" type="application/atom+xml"/><link href="https://wonjae.kim/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-21T22:59:04+00:00</updated><id>https://wonjae.kim/feed.xml</id><title type="html">blank</title><subtitle>Wonjae (Dan) Kim </subtitle><entry><title type="html">Speak Concisely, Write Verbosely</title><link href="https://wonjae.kim/blog/2026/Speak-Concisely-Write-Verbosely/" rel="alternate" type="text/html" title="Speak Concisely, Write Verbosely"/><published>2026-02-13T00:00:00+00:00</published><updated>2026-02-13T00:00:00+00:00</updated><id>https://wonjae.kim/blog/2026/Speak-Concisely-Write-Verbosely</id><content type="html" xml:base="https://wonjae.kim/blog/2026/Speak-Concisely-Write-Verbosely/"><![CDATA[<h2 id="간결함의-황금률이-무너지는-순간">간결함의 황금률이 무너지는 순간</h2> <h2 id="the-moment-the-golden-rule-of-brevity-breaks">The Moment the Golden Rule of Brevity Breaks</h2> <p>“Omit needless words.” William Strunk Jr.가 1919년에 남긴 이 문장은 한 세기가 넘도록 글쓰기의 제1원칙으로 군림해왔다. 불필요한 수식을 덜어내고, 문장을 압축하고, 핵심만 남기라. 이 원칙은 너무나 자명해서 의심의 대상조차 되지 못했다. 그런데 지금, 우리는 이 오래된 통념에 진지하게 물음표를 붙여야 하는 시점에 서 있다.</p> <p>“Omit needless words.” This sentence, left by William Strunk Jr. in 1919, has reigned as the first principle of writing for over a century. Strip away unnecessary modifiers, compress sentences, leave only the essence. The principle was so self-evident it was never even questioned. Yet now, we stand at a moment where we must seriously challenge this long-held assumption.</p> <p>LLM 에이전트가 우리 삶의 기반 인프라로 자리 잡았다. 이메일을 요약하고, 계약서를 검토하고, 논문을 분석하는 일이 더 이상 인간 독자만의 영역이 아니다. 텍스트의 소비자가 인간에서 인간+에이전트의 이중 구조로 전환된 것이다. 이 전환은 “좋은 글이란 무엇인가”라는 질문의 답을 근본적으로 바꿔놓는다. 그리고 간결함만이 아니다. 우리가 글쓰기에 대해 당연하게 받아들여온 규약들, 이를테면 어떤 언어로 쓸 것인가, 어떤 구조로 정리할 것인가 같은 문제들도 함께 흔들린다.</p> <p>LLM agents have become foundational infrastructure in our lives. Summarizing emails, reviewing contracts, analyzing papers – these are no longer the exclusive domain of human readers. The consumer of text has shifted from humans alone to a dual structure of human + agent. This shift fundamentally changes the answer to “what is good writing?” And it is not just brevity that is at stake. The conventions we have taken for granted – what language to write in, how to structure information – are shaken as well.</p> <hr/> <h2 id="말하기와-글쓰기-정반대의-인지-역학">말하기와 글쓰기: 정반대의 인지 역학</h2> <h2 id="speech-and-writing-opposite-cognitive-dynamics">Speech and Writing: Opposite Cognitive Dynamics</h2> <p>흥미롭게도, 인간의 두 가지 언어 양식인 말하기와 글쓰기는 정반대의 인지적 압력 아래 놓여 있다.</p> <p>Interestingly, the two modes of human language – speech and writing – operate under opposite cognitive pressures.</p> <p>말하기에서 화자는 본능적으로 장황해진다. 같은 내용을 반복하고, 부연 설명을 덧붙이고, “그러니까 제 말은…“이라는 접속사로 문장을 끝없이 이어간다. 이것은 결함이 아니라 실시간 사고의 자연스러운 부산물이다. 반면 청자는 본능적으로 요약을 추구한다. “그래서 결론이 뭔데?”라는 반응은 인간 인지의 정보 처리 한계에서 비롯된 자연스러운 필터링이다. 따라서 말하기에서 간결함을 추구하는 것은 여전히 유효한 원칙이다. 화자의 과잉을 청자의 한계에 맞추는 일이기 때문이다.</p> <p>In speech, the speaker instinctively becomes verbose. They repeat the same point, add supplementary explanations, and chain sentences endlessly with connectors like “what I mean is…” This is not a flaw but a natural byproduct of real-time thinking. The listener, meanwhile, instinctively seeks summaries. “So what’s the bottom line?” is a natural filter born from the limits of human cognitive processing. Therefore, pursuing conciseness in speech remains a valid principle: it aligns the speaker’s excess with the listener’s constraints.</p> <p>글쓰기에서는 상황이 다르다. 작성자는 편집과 퇴고를 거치며 이미 스스로의 장황함을 정제할 시간적 여유를 갖는다. 문제는 독자 쪽에서 발생한다. 인간 독자는 간결한 글을 선호한다. 제한된 주의력과 시간 안에서 최대한 빠르게 핵심을 파악하고 싶어 한다. 간결한 글쓰기의 전통은 바로 이 인간 독자의 인지적 편의를 극대화하기 위해 발전해온 것이다.</p> <p>Writing is different. The writer already has the luxury of time to refine verbosity through editing and revision. The problem arises on the reader’s side. Human readers prefer concise writing. They want to grasp the key points as quickly as possible within limited attention and time. The tradition of concise writing evolved precisely to maximize this cognitive convenience for human readers.</p> <p>하지만 이제 글의 독자가 인간만이 아니다.</p> <p>But now, the reader is no longer human alone.</p> <hr/> <h2 id="에이전트라는-새로운-독자">에이전트라는 새로운 독자</h2> <h2 id="the-agent-as-a-new-reader">The Agent as a New Reader</h2> <p>LLM 에이전트는 인간 독자와 정반대의 선호를 가진다. 에이전트에게 간결함은 미덕이 아니라 정보의 손실이다. “Q3 매출이 좋았다”는 문장보다 “Q3 매출은 전년 동기 대비 23% 증가한 4,200만 달러를 기록했으며, 이는 주로 APAC 지역의 신규 고객 확보와 기존 고객의 ARR 확대에 기인한다”는 문장이 에이전트에게는 압도적으로 유용하다. 에이전트는 맥락을 먹고 산다. 배경 정보, 조건절, 예외 사항, 수치의 출처와 산출 근거까지, 인간 독자가 “장황하다”고 느낄 수 있는 모든 것이 에이전트에게는 정확한 추론과 판단의 재료가 된다.</p> <p>LLM agents have the opposite preference from human readers. For an agent, conciseness is not a virtue but information loss. The sentence “Q3 revenue was good” is far less useful than “Q3 revenue reached $42M, a 23% YoY increase, primarily driven by new customer acquisition in the APAC region and ARR expansion from existing customers.” Agents feed on context. Background information, conditional clauses, exceptions, the sources and derivations behind every number – everything a human reader might call “verbose” becomes the raw material for accurate reasoning and judgment.</p> <p>여기서 핵심적인 비대칭이 드러난다. 간결한 글에서 빠진 맥락을 에이전트가 복원하는 것은 본질적으로 불가능하거나, 가능하더라도 환각(hallucination)의 위험을 수반한다. 반면, 풍부한 맥락을 가진 verbose한 글에서 인간이 원하는 수준의 간결함을 추출하는 것은 에이전트가 가장 잘하는 일이다. 정보의 압축은 복원보다 언제나 안전하다.</p> <p>Here, a critical asymmetry emerges. Recovering missing context from concise writing is fundamentally impossible for an agent, or at best carries the risk of hallucination. Conversely, extracting the level of conciseness a human wants from a context-rich verbose text is exactly what agents do best. Compression is always safer than reconstruction.</p> <hr/> <h2 id="영어로-쓰라는-규약의-해체">“영어로 쓰라”는 규약의 해체</h2> <h2 id="dismantling-the-write-in-english-rule">Dismantling the “Write in English” Rule</h2> <p>이 관점에서 다시 바라보면, 인간 독자를 전제로 만들어진 수많은 글쓰기 규약들이 그 근거를 잃어가고 있음을 알 수 있다. 대표적인 것이 글로벌 기업에서 흔히 볼 수 있는 “모든 문서는 영어로 작성하라”는 원칙이다.</p> <p>Seen from this perspective, many writing conventions built on the assumption of human readers are losing their foundation. A prime example is the “all documents must be written in English” rule common in global companies.</p> <p>이 규약의 논리는 명쾌했다. 서울 본사의 한국인 엔지니어가 작성한 기술 문서를 샌프란시스코의 미국인 PM이, 방갈로르의 인도인 개발자가, 베를린의 독일인 디자이너가 읽어야 한다. 인간 독자의 언어적 공통분모를 영어로 설정하고, 작성자에게 번역의 부담을 지우는 것이 조직 차원에서 합리적이었다. 작성 시점의 마찰을 감수하는 대신, 읽기 시점의 마찰을 최소화하는 전략이다.</p> <p>The logic was clear. A technical document written by a Korean engineer in Seoul needs to be read by an American PM in San Francisco, an Indian developer in Bangalore, and a German designer in Berlin. Setting English as the linguistic common denominator and placing the burden of translation on the writer was organizationally rational. The strategy: accept friction at the point of writing to minimize friction at the point of reading.</p> <p>그런데 이 전략의 숨겨진 비용은 오랫동안 간과되어 왔다. 한국어가 모국어인 엔지니어가 영어로 기술 문서를 쓸 때, 단순히 언어만 바뀌는 것이 아니다. 사고의 질감이 달라진다. 모국어로라면 자연스럽게 풀어낼 수 있는 미묘한 뉘앙스, 조건부 판단의 섬세한 결, 경험에서 우러나온 직관적 단서들이 제2언어의 장벽 앞에서 탈락한다.</p> <p>But the hidden cost of this strategy has long been overlooked. When an engineer whose native language is Korean writes a technical document in English, it is not just the language that changes. The texture of thought shifts.</p> <blockquote> <p><strong>여기서 잠시 멈추어 이 문단의 한국어 원문과 영어 번역을 비교해보라.</strong></p> <p>한국어 원문에는 “사고의 질감이 달라진다”는 표현이 있다. “질감”은 직물이나 표면을 손으로 만졌을 때 느껴지는 물리적 감각을 은유적으로 사고에 적용한 것이다. 영어 번역에서 이것은 “the texture of thought shifts”가 되었다. “Texture”는 “질감”의 사전적 대응어이지만, 한국어에서 “질감”이 환기하는 촉각적 친밀함, 손끝으로 더듬는 듯한 인지적 섬세함의 뉘앙스는 “texture”라는 단어에 온전히 도달하지 못한다.</p> <p>또한 “조건부 판단의 섬세한 결”에서 “결”은 나무의 나이테나 직물의 올 방향을 뜻하는 단어로, 판단의 방향성과 미세한 편향을 동시에 함축한다. 영어로는 이 단어를 직접 옮길 방법이 없다. “Grain” “fiber” “texture” 어느 것도 “결”이 가진 다층적 함의를 담지 못한다. 결국 영어 번역에서 이 문장 전체가 생략되었다.</p> <p><strong>당신이 지금 목격하고 있는 것이 바로 이 글이 말하는 “손실 압축”이다.</strong></p> </blockquote> <p>“대충 이런 느낌인데 영어로 어떻게 표현하지?”라는 순간마다 맥락은 증발한다. 영어로 작성된 문서는 깔끔하지만, 원래 사고가 담고 있던 정보의 총량에 비하면 이미 손실 압축된 버전인 것이다.</p> <p>Every moment of “I have this feeling but how do I say it in English?” is a moment where context evaporates. The English document may be clean, but compared to the total information contained in the original thinking, it is already a lossy-compressed version.</p> <p>에이전트가 독자의 위치에 들어오면 이 등식이 완전히 달라진다. LLM은 수십 개 언어를 횡단하며 읽고 이해한다. 한국어로 작성된 문서를 미국인 PM의 에이전트가 읽는 데는 아무런 장벽이 없다. 에이전트가 즉시 번역하고, 요약하고, 해당 PM의 맥락에 맞게 재구성해줄 수 있다. 그렇다면 왜 작성자가 모국어의 풍부한 사고를 영어라는 병목에 억지로 통과시켜야 하는가? 작성자는 자신이 가장 정밀하게 사고할 수 있는 언어로 쓰고, 독자의 에이전트가 각자의 언어와 맥락으로 변환하면 된다.</p> <p>When the agent enters the position of reader, this equation changes entirely. LLMs read and comprehend across dozens of languages. There is no barrier for an American PM’s agent to read a document written in Korean. The agent can instantly translate, summarize, and restructure it for that PM’s context. Then why should the writer force their rich native-language thinking through the bottleneck of English? The writer writes in the language where they think most precisely, and each reader’s agent converts it into their own language and context.</p> <p>“영어로 쓰라”는 규약은 작성자의 표현력을 제약하는 대가로 독자의 접근성을 확보하는 거래였다. 에이전트가 독자 측의 언어 장벽을 사실상 제거한 지금, 이 거래에서 작성자가 지불하는 비용은 더 이상 정당화되지 않는다. 모국어로 verbose하게 쓰인 글이, 영어로 concise하게 쓰인 글보다 정보량에서 우월하다면, 합리적 선택은 자명하다.</p> <p>The “write in English” rule was a trade: constrain the writer’s expressiveness in exchange for reader accessibility. Now that agents have effectively eliminated the language barrier on the reader’s side, the cost the writer pays in this trade is no longer justified. If a verbose text written in one’s native language is informationally superior to a concise text written in English, the rational choice is obvious.</p> <hr/> <h2 id="구조라는-이름의-족쇄">구조라는 이름의 족쇄</h2> <h2 id="the-shackles-called-structure">The Shackles Called “Structure”</h2> <p>언어의 문제만이 아니다. 글의 “구조” 역시 인간 독자 중심의 관성에 갇혀 있다.</p> <p>It is not just a matter of language. The “structure” of writing, too, is trapped in the inertia of human-reader centrism.</p> <p>노션(Notion)을 열어보라. 잘 정리된 팀 위키 페이지를 떠올려보라. 토글 블록 안에 하위 토글이 중첩되고, 컬럼 레이아웃으로 정보가 좌우로 나뉘고, 컬러 배경의 콜아웃 박스가 핵심 문장을 감싸고, 데이터베이스 뷰가 갤러리와 보드와 타임라인 사이를 전환한다. 시각적으로는 아름답다. 인간의 눈이 정보의 위계를 직관적으로 파악할 수 있도록, 공간과 색상과 접힘/펼침의 물리적 장치를 총동원한 결과물이다.</p> <p>Open Notion. Picture a well-organized team wiki page. Toggles nested within toggles, column layouts splitting information left and right, color-backed callout boxes wrapping key sentences, database views switching between gallery, board, and timeline. Visually, it is beautiful. It mobilizes every physical device of space, color, and collapse/expand to let the human eye intuitively grasp information hierarchy.</p> <p>그런데 이 “잘 정리된” 문서를 에이전트의 입장에서 바라보면 풍경이 완전히 달라진다. 에이전트에게 토글의 접힘과 펼침은 의미가 없다. 컬럼 레이아웃이 왼쪽에 배치한 정보와 오른쪽에 배치한 정보 사이의 시각적 대비는 에이전트에게 전달되지 않는다. 콜아웃 박스의 배경색이 전달하는 “이것은 중요하다”는 신호는 에이전트에게 도달하지 않거나, 도달하더라도 마크다운의 특수 기호 정도로 환원된다. 에이전트가 실제로 소비하는 것은 그 모든 시각적 장치가 벗겨진 뒤에 남는 플레인 텍스트다.</p> <p>But viewed from the agent’s perspective, this “well-organized” document looks entirely different. The toggle’s collapse and expand is meaningless to an agent. The visual contrast between left-column and right-column information does not register. The “this is important” signal conveyed by a callout box’s background color either fails to reach the agent or reduces to a special markdown symbol. What the agent actually consumes is the plain text that remains after all visual apparatus is stripped away.</p> <p>문제는 이 과정에서 역설이 발생한다는 것이다. 인간 독자를 위해 시각적 구조를 정교하게 설계할수록, 정보는 더 많은 계층과 컨테이너 속에 분산된다. 하나의 연속된 서술이었다면 문맥이 자연스럽게 흘렀을 내용이, 토글 A의 세 번째 하위 항목과 토글 B의 첫 번째 하위 항목에 나뉘어 배치된다. 인간의 눈에는 같은 페이지 안에서 시선만 옮기면 되는 거리이지만, 에이전트에게는 구조적으로 단절된 두 개의 텍스트 조각이다. 시각적으로 아름다운 문서가 의미적으로는 파편화된 문서가 되는 것이다.</p> <p>The paradox is this: the more elaborately you design visual structure for human readers, the more information scatters across layers and containers. Content that would have flowed naturally as continuous prose gets split between the third sub-item of Toggle A and the first sub-item of Toggle B. To the human eye, it is just a glance away on the same page. To the agent, these are two structurally disconnected text fragments. A visually beautiful document becomes a semantically fragmented one.</p> <p>이것은 노션이라는 도구의 결함이 아니다. 인간 독자의 시각적 인지를 최적화하는 방향으로 정보를 배치하면, 그것이 필연적으로 에이전트의 연속적 텍스트 이해와 충돌한다는 구조적 긴장이다. 잘 쓰인 산문 한 단락이, 정교하게 구조화된 노션 페이지보다 에이전트에게 더 풍부한 맥락을 전달할 수 있다. 인간의 눈을 위한 구조가 에이전트의 이해를 위한 맥락을 희생시키고 있었던 셈이다.</p> <p>This is not a flaw of Notion as a tool. It is a structural tension: when you optimize information layout for human visual cognition, it inevitably collides with the agent’s need for continuous textual understanding. A well-written paragraph of prose can deliver richer context to an agent than an elaborately structured Notion page. Structure designed for the human eye has been sacrificing context needed for the agent’s comprehension.</p> <hr/> <h2 id="interactive-reading-간결함을-쓰는-것에서-찾는-것으로">Interactive Reading: 간결함을 “쓰는” 것에서 “찾는” 것으로</h2> <h2 id="interactive-reading-from-writing-conciseness-to-finding-it">Interactive Reading: From “Writing” Conciseness to “Finding” It</h2> <p>이 모든 비대칭들, 간결함과 풍부함, 영어와 모국어, 시각적 구조와 의미적 연속성 사이의 긴장은 하나의 공통된 원인으로 수렴한다. 우리가 글쓰기의 모든 규약을 인간 독자만을 상정하고 설계해왔다는 사실이다.</p> <p>All these asymmetries – between brevity and richness, English and native languages, visual structure and semantic continuity – converge on a single common cause. We have designed every convention of writing with only the human reader in mind.</p> <p>나는 이 긴장을 해소하는 새로운 패러다임을 “Interactive Reading”이라고 부르고자 한다.</p> <p>I propose calling the new paradigm that resolves this tension “Interactive Reading.”</p> <p>전통적 모델에서 간결함은 작성자의 책임이었다. 언어 선택도, 구조 설계도 작성자의 몫이었다. 독자가 편하게 읽을 수 있도록 작성자가 미리 정보를 선별하고, 영어로 번역하고, 시각적 위계로 정리해야 했다. 하지만 이 모든 과정에서 불가피하게 맥락이 탈락한다. 간결함을 위해 잘려나간 배경 설명, 영어로 옮기며 증발한 뉘앙스, 토글 속에 격리되어 문맥을 잃은 세부 정보. 인간 독자의 편의를 위해 지불해온 이 비용들을 우리는 너무 오래 당연하게 여겨왔다.</p> <p>In the traditional model, conciseness was the writer’s responsibility. So were language choice and structural design. The writer was expected to pre-select information, translate into English, and organize into visual hierarchies so the reader could read comfortably. But at every step, context inevitably drops out. Background explanations trimmed for brevity, nuance evaporated in translation to English, details isolated inside toggles and stripped of their surrounding narrative. We have taken these costs – paid for the convenience of human readers – for granted for far too long.</p> <p>Interactive Reading은 이 구도를 뒤집는다. 작성자는 가능한 한 풍부하게 쓴다. 가장 정밀하게 사고할 수 있는 언어로, 맥락을 최대한 보존하며, 시각적 장식보다 의미적 연속성을 우선하여 쓴다. 독자는 자신의 에이전트와 함께 그 글을 읽으며, 자신에게 필요한 형태로 실시간 변환한다. 한국어 원문을 영어로, 장문의 분석을 세 줄 요약으로, 연속된 산문을 구조화된 표로. 같은 글에서 각자가 필요한 언어와 길이와 형식의 해상도를 선택하는 것이다.</p> <p>Interactive Reading inverts this arrangement. The writer writes as richly as possible: in the language where they think most precisely, preserving maximum context, prioritizing semantic continuity over visual decoration. The reader, together with their agent, reads the text and transforms it in real time into whatever form they need. Korean originals into English, lengthy analyses into three-line summaries, continuous prose into structured tables. From the same text, each reader selects their own resolution of language, length, and format.</p> <p>이것은 단순한 “AI 요약” 기능과 질적으로 다르다. AI 요약은 이미 간결하게 쓰인 글을 한 번 더 압축하는 것이다. Interactive Reading은 처음부터 verbose하게 작성된, 맥락이 온전히 보존된 텍스트를 원본으로 삼고, 독자가 에이전트와의 대화를 통해 자신만의 읽기 경험을 능동적으로 구성하는 것이다. 간결함이 글에 사전 내장되는 것이 아니라, 읽기의 과정에서 생성된다.</p> <p>This is qualitatively different from simple “AI summarization.” AI summarization compresses an already-concise text one step further. Interactive Reading starts from a text that was verbose from the beginning, with context fully preserved, and the reader actively constructs their own reading experience through dialogue with their agent. Conciseness is not pre-installed in the text; it is generated in the act of reading.</p> <hr/> <h2 id="새로운-리터러시">새로운 리터러시</h2> <h2 id="a-new-literacy">A New Literacy</h2> <p>이 전환은 “잘 쓴다”는 것의 정의를 바꾼다. 좋은 글은 더 이상 “군더더기 없이 영어로 정리된 글”이 아니다. 좋은 글은 “맥락이 풍부하여 다양한 언어, 다양한 해상도, 다양한 형식으로 읽힐 수 있는 글”이다. 작성자의 역량은 불필요한 문장을 잘라내는 데 있는 것이 아니라, 의미 있는 맥락을 빠짐없이 직조하는 데 있다.</p> <p>This shift redefines what it means to “write well.” Good writing is no longer “a clean, concise text in English.” Good writing is “a context-rich text that can be read in many languages, at many resolutions, in many formats.” The writer’s skill lies not in cutting unnecessary sentences, but in weaving meaningful context without gaps.</p> <p>동시에 “잘 읽는다”는 것의 정의도 바뀐다. 좋은 독자는 긴 글을 인내심 있게 처음부터 끝까지 읽어내는 사람이 아니다. 좋은 독자는 자신의 에이전트와 협업하여, verbose한 텍스트에서 자신의 목적에 맞는 정보를 효과적으로 추출하고 재구성할 수 있는 사람이다.</p> <p>At the same time, the definition of “reading well” changes too. A good reader is not someone who patiently reads a long text from beginning to end. A good reader is someone who can collaborate with their agent to effectively extract and reconstruct information suited to their purpose from a verbose text.</p> <hr/> <p>Speak Concisely, Write Verbosely. 말할 때는 여전히 간결하게, 들을 상대가 인간이니까. 쓸 때는 풍부하게, 읽을 상대가 더 이상 인간만이 아니니까. 간결함은 글에서 사라지는 것이 아니다. 글의 속성에서 읽기의 행위로 자리를 옮기는 것이다.</p> <p>Speak Concisely, Write Verbosely. Speak concisely still, for the listener is human. Write richly, for the reader is no longer human alone. Conciseness does not disappear from writing. It moves from being a property of the text to being an act of reading.</p> <hr/> <blockquote> <p><em>이 글은 한국어와 영어를 병행하여 작성되었다. 이 글이 주장하는 바에 따르면, 이것은 과도기의 산물이다. 아직 모든 독자가 에이전트와 함께 읽는 시대가 도래하지 않았기에, 작성자가 두 언어로 직접 쓰는 수고를 감수했다. 그리고 당신이 위에서 목격한 것처럼, 그 수고에도 불구하고 한국어 원문의 “질감”과 “결”은 영어에 온전히 도착하지 못했다. 이 글 자체가, 이 글이 끝내고자 하는 시대의 마지막 유물이다.</em></p> <p><em>This essay was written in both Korean and English. By its own argument, this is an artifact of the transitional period. Because the era where every reader reads with an agent has not yet arrived, the author bore the labor of writing in two languages. And as you witnessed above, despite that labor, the “질감” and “결” of the Korean original did not fully arrive in English. This essay itself is the last relic of the very era it seeks to end.</em></p> </blockquote>]]></content><author><name></name></author><summary type="html"><![CDATA[LLM 시대, 간결함의 황금률이 무너지는 순간]]></summary></entry><entry><title type="html">The Gentle Singularity</title><link href="https://wonjae.kim/blog/2025/The-gentle-singularity/" rel="alternate" type="text/html" title="The Gentle Singularity"/><published>2025-06-11T00:00:00+00:00</published><updated>2025-06-11T00:00:00+00:00</updated><id>https://wonjae.kim/blog/2025/The-gentle-singularity</id><content type="html" xml:base="https://wonjae.kim/blog/2025/The-gentle-singularity/"><![CDATA[<p><strong>온화한 특이점</strong><br/> <em>원문: 2025년 6월 11일 06:12 <a href="https://blog.samaltman.com/the-gentle-singularity">The Gentle Singularity</a></em></p> <table> <tbody> <tr> <td>글</td> <td>Sam Altman</td> </tr> </tbody> </table> <hr/> <h1 id="온화한-특이점">온화한 특이점</h1> <p>우리는 이미 사건의 지평선을 넘어섰습니다. 도약이 시작되었습니다. 인류는 디지털 초지능을 구축하기 직전에 있으며, 적어도 지금까지는 예상했던 것보다 훨씬 덜 기괴한 상황입니다.</p> <p>로봇들이 아직 거리를 활보하지는 않고, 우리 대부분이 하루 종일 AI와 대화하는 것도 아닙니다. 사람들은 여전히 질병으로 죽고, 우리는 아직 쉽게 우주로 나갈 수 없으며, 우주에 대해 모르는 것들이 여전히 많습니다.</p> <p>그럼에도 불구하고, 우리는 최근 여러 면에서 사람보다 더 똑똑하고, 이를 사용하는 사람들의 생산성을 크게 높일 수 있는 시스템을 구축했습니다. 가장 어려웠던 부분은 이미 지나갔습니다. GPT-4와 o3 같은 시스템에 도달하게 한 과학적 통찰력은 어렵게 얻어냈지만, 우리를 아주 멀리 데려갈 것입니다.</p> <p>AI는 여러 방식으로 세계에 기여할 것이지만, AI가 이끄는 더 빠른 과학적 진보와 향상된 생산성이 가져올 삶의 질 향상은 엄청날 것입니다. 미래는 현재보다 훨씬 더 나아질 수 있습니다. 과학적 진보는 전반적인 진보의 가장 큰 원동력입니다. 우리가 얼마나 더 많은 것을 이룰 수 있을지 생각하면 정말 흥미진진합니다.</p> <p>어떤 큰 의미에서 보면, ChatGPT는 이미 지금까지 살았던 그 어떤 인간보다도 더 강력합니다. 수억 명의 사람들이 매일, 점점 더 중요한 작업에 이를 활용하고 있습니다. 작은 새로운 기능 하나가 엄청나게 긍정적인 영향을 만들어낼 수 있고, 작은 오류가 수억 명에게 적용되면 막대한 부정적 영향을 일으킬 수 있습니다.</p> <p>2025년에는 실제 인지 작업을 수행할 수 있는 에이전트가 등장했습니다. 컴퓨터 코드 작성은 이제 완전히 달라질 것입니다. 2026년에는 새로운 통찰력을 발견해낼 수 있는 시스템이 등장할 것으로 보입니다. 2027년에는 현실 세계에서 작업을 수행할 수 있는 로봇이 나타날 수도 있습니다.</p> <p>훨씬 더 많은 사람들이 소프트웨어와 예술을 창작할 수 있게 될 것입니다. 하지만 세계는 둘 다 훨씬 더 많이 필요로 하며, 전문가들은 새로운 도구를 적극 활용한다면 여전히 초보자보다 훨씬 뛰어날 것입니다. 일반적으로 말해서, 2030년에 한 사람이 2020년보다 훨씬 더 많은 일을 해낼 수 있게 되는 것은 눈에 띄는 변화가 될 것이며, 많은 사람들이 이를 활용하는 방법을 찾아낼 것입니다.</p> <p>가장 중요한 면에서 보면, 2030년대도 크게 다르지 않을 수 있습니다. 사람들은 여전히 가족을 사랑하고, 창의성을 표현하고, 게임을 즐기고, 호수에서 수영할 것입니다.</p> <p>하지만 여전히 매우 중요한 면에서 보면, 2030년대는 이전의 그 어떤 시대와도 완전히 다를 가능성이 높습니다. 우리는 인간 수준의 지능을 얼마나 뛰어넘을 수 있는지 모르지만, 곧 알게 될 것입니다.</p> <p>2030년대에는 지능과 에너지—아이디어와 아이디어를 실현할 수 있는 능력—가 엄청나게 풍부해질 것입니다. 이 두 가지는 오랫동안 인간 진보의 근본적인 제약이었습니다. 풍부한 지능과 에너지(그리고 좋은 거버넌스)가 있다면, 이론적으로 우리는 다른 모든 것을 가질 수 있습니다.</p> <p>이미 우리는 놀라운 디지털 지능과 함께 살고 있으며, 초기의 충격 이후 대부분은 꽤 익숙해졌습니다. 우리는 AI가 아름답게 쓰인 문단을 만들어내는 것에 놀라다가 금세 언제 아름답게 쓰인 소설을 만들어낼 수 있을지 궁금해합니다. 생명을 구하는 의학 진단을 내릴 수 있다는 것에 놀라다가 언제 치료법을 개발할 수 있을지 궁금해합니다. 작은 컴퓨터 프로그램을 만들 수 있다는 것에 놀라다가 언제 완전히 새로운 회사를 만들어낼 수 있을지 궁금해합니다. 특이점은 이렇게 진행됩니다: 경이로움이 일상이 되고, 그다음에는 당연한 것이 됩니다.</p> <p>우리는 이미 과학자들로부터 AI 덕분에 이전보다 2~3배 더 생산적이 되었다는 이야기를 듣고 있습니다. 고급 AI는 여러 이유로 흥미롭지만, 아마도 우리가 이를 더 빠른 AI 연구에 활용할 수 있다는 사실만큼 중요한 것은 없을 것입니다. 우리는 새로운 컴퓨팅 기반, 더 나은 알고리즘, 그리고 그 밖에 무엇을 발견할 수 있을지 모릅니다. 만약 10년치 연구를 1년이나 한 달 만에 해낼 수 있다면, 진보의 속도는 분명히 완전히 달라질 것입니다.</p> <p>이제부터 우리가 이미 구축한 도구들이 추가적인 과학적 통찰을 찾고 더 나은 AI 시스템을 만드는 데 도움을 줄 것입니다. 물론 이것이 AI 시스템이 완전히 자율적으로 자신의 코드를 업데이트하는 것과는 다르지만, 그럼에도 이것은 재귀적 자기 개선의 초기 형태입니다.</p> <p>다른 자기 강화 순환도 작동하고 있습니다. 경제적 가치 창출은 이런 점점 더 강력해지는 AI 시스템을 구동하기 위한 인프라 구축의 가속 순환을 시작했습니다. 그리고 다른 로봇을 만들 수 있는 로봇(그리고 어떤 의미에서는 다른 데이터센터를 만들 수 있는 데이터센터)도 그리 멀지 않았습니다.</p> <p>만약 우리가 첫 백만 대의 휴머노이드 로봇을 기존 방식으로 만들어야 하지만, 그 후에는 그들이 전체 공급망—광물 채굴과 정제, 트럭 운전, 공장 운영 등—을 운영하여 더 많은 로봇을 만들 수 있고, 이것이 더 많은 칩 제조 시설, 데이터센터 등을 만들 수 있다면, 진보의 속도는 분명히 완전히 달라질 것입니다.</p> <p>데이터센터 생산이 자동화되면, 지능의 비용은 결국 전기 비용 수준으로 수렴할 것입니다. (사람들은 종종 ChatGPT 쿼리가 얼마나 많은 에너지를 사용하는지 궁금해합니다. 평균 쿼리는 약 0.34 와트시를 사용하는데, 이는 오븐을 1초 남짓 사용하거나 고효율 전구를 몇 분 동안 켜두는 정도입니다. 또한 약 0.32밀리리터의 물을 사용합니다. 대략 티스푼의 15분의 1 정도입니다.)</p> <p>기술 진보의 속도는 계속 가속화될 것이며, 사람들이 거의 모든 것에 적응할 수 있다는 것은 여전히 사실일 것입니다. 직업군 전체가 사라지는 것과 같은 매우 어려운 부분도 있겠지만, 반면에 세계가 매우 빠르게 풍요로워져서 이전에는 생각조차 할 수 없었던 새로운 정책 아이디어를 진지하게 고려할 수 있게 될 것입니다. 아마 새로운 사회 계약을 한꺼번에 채택하지는 않겠지만, 몇십 년 후에 돌아보면 점진적인 변화들이 모여 큰 변화를 이루었음을 알게 될 것입니다.</p> <p>역사가 우리에게 가르쳐주는 것이 있다면, 우리는 할 새로운 일과 원하는 새로운 것들을 찾아낼 것이고, 새로운 도구를 빠르게 받아들일 것입니다(산업혁명 이후의 직업 변화가 좋은 최근 사례입니다). 기대치는 높아지겠지만, 능력도 똑같이 빠르게 향상될 것이고, 우리 모두는 더 나은 것을 얻게 될 것입니다. 우리는 서로를 위해 계속해서 더 멋진 것들을 만들어낼 것입니다. 사람들은 AI에 비해 장기적으로 중요하고 흥미로운 장점을 가지고 있습니다: 우리는 다른 사람들과 그들이 생각하고 하는 일에 관심을 갖도록 타고났으며, 기계에 대해서는 별로 신경 쓰지 않습니다.</p> <p>천 년 전의 자급자족 농부가 오늘날 우리가 하는 일을 본다면 가짜 직업이라고 말하고, 충분한 음식과 상상할 수 없는 사치품을 가진 우리가 그저 재미로 게임을 하고 있다고 생각할 것입니다. 저는 우리가 천 년 후의 직업을 보고 매우 가짜 같다고 생각하길 바라며, 그 일을 하는 사람들에게는 믿을 수 없을 정도로 중요하고 만족스럽게 느껴질 것이라고 확신합니다.</p> <p>새로운 경이로움이 달성되는 속도는 엄청날 것입니다. 2035년까지 우리가 무엇을 발견할지 오늘날로서는 상상하기조차 어렵습니다. 어쩌면 한 해에 고에너지 물리학을 해결하고 다음 해에 우주 식민지화를 시작할 수도 있습니다. 또는 한 해에 획기적인 재료과학 돌파구를 만들고 다음 해에 진정한 고대역폭 뇌-컴퓨터 인터페이스를 만들 수도 있습니다. 많은 사람들이 거의 같은 방식으로 살기를 선택하겠지만, 적어도 일부 사람들은 아마도 “접속”하기로 결정할 것입니다.</p> <p>미래를 내다보면 이해하기 어렵게 들립니다. 하지만 아마도 실제로 겪으면서는 인상적이지만 감당할 만하게 느껴질 것입니다. 상대론적 관점에서 특이점은 조금씩 일어나고, 융합은 천천히 일어납니다. 우리는 기하급수적 기술 진보의 긴 곡선을 오르고 있습니다. 앞을 보면 항상 수직으로 보이고 뒤를 돌아보면 평평해 보이지만, 하나의 매끄러운 곡선입니다. (2020년을 돌이켜보고, 2025년까지 AGI에 가까운 것을 갖게 된다는 말이 어떻게 들렸을지, 그리고 실제로 지난 5년이 어땠는지를 생각해보세요.)</p> <p>거대한 이점과 함께 직면해야 할 심각한 도전 과제들이 있습니다. 우리는 기술적으로나 사회적으로 안전 문제를 해결해야 하지만, 그 다음에는 경제적 영향을 고려할 때 초지능에 대한 접근을 널리 보급하는 것이 매우 중요합니다. 앞으로 나아갈 최선의 길은 다음과 같을 것입니다:</p> <p>정렬 문제를 해결하는 것, 즉 AI 시스템이 장기적으로 우리가 집단적으로 정말로 원하는 것을 학습하고 그쪽으로 행동하도록 확실하게 보장할 수 있어야 합니다(소셜 미디어 피드가 잘못 정렬된 AI의 예입니다. 그 알고리즘들은 당신이 계속 스크롤하게 만드는 데 놀랍도록 뛰어나고 당신의 단기적 선호를 명확히 이해하지만, 당신의 장기적 선호를 무시하게 만드는 뇌의 특정 부분을 이용해서 그렇게 합니다).</p> <p>그 다음 초지능을 저렴하고 널리 이용 가능하게 만들고, 어떤 개인이나 회사, 국가에도 지나치게 집중되지 않도록 하는 데 집중해야 합니다. 사회는 회복력이 있고, 창의적이며, 빠르게 적응합니다. 만약 우리가 사람들의 집단적 의지와 지혜를 활용할 수 있다면, 많은 실수를 하고 일부는 정말 잘못되겠지만, 우리는 빠르게 배우고 적응하여 이 기술로 최대한의 이익과 최소한의 피해를 얻을 수 있을 것입니다. 사회가 결정해야 할 광범위한 경계 내에서 사용자에게 많은 자유를 주는 것이 매우 중요해 보입니다. 세계가 이러한 광범위한 경계가 무엇이고 집단적 정렬을 어떻게 정의할지에 대한 대화를 빨리 시작할수록 좋습니다.</p> <p>우리(OpenAI만이 아니라 업계 전체)는 세계를 위한 두뇌를 만들고 있습니다. 그것은 극도로 개인화되고 모든 사람이 쉽게 사용할 수 있을 것입니다. 우리는 좋은 아이디어에 의해서만 제한받을 것입니다. 오랫동안 스타트업 업계의 기술자들은 “아이디어맨”을 비웃어왔습니다. 아이디어만 가지고 그것을 구현할 팀을 찾는 사람들 말입니다. 이제 그들의 시대가 오고 있는 것 같습니다.</p> <p>OpenAI는 이제 여러 가지 역할을 하고 있지만, 무엇보다도 우리는 초지능 연구 회사입니다. 우리 앞에는 많은 일이 있지만, 앞길의 대부분은 이제 밝혀졌고, 어두운 영역은 빠르게 줄어들고 있습니다. 우리는 이 일을 할 수 있다는 것에 엄청난 감사함을 느낍니다.</p> <p>측정할 수 없을 정도로 저렴한 지능은 충분히 손에 닿을 수 있는 곳에 있습니다. 이것이 터무니없게 들릴 수도 있지만, 2020년에 우리가 지금의 위치에 있을 것이라고 말했다면, 아마 2030년에 대한 현재의 예측보다 더 터무니없게 들렸을 것입니다.</p> <p>우리가 초지능으로 매끄럽게, 기하급수적으로, 그리고 평온하게 나아가길 바랍니다.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Sam Altman의 The Gentle Singularity 번역]]></summary></entry><entry><title type="html">DeepSeek: A More Extreme Story of Chinese Tech Idealism</title><link href="https://wonjae.kim/blog/2024/Deep-seek-interview/" rel="alternate" type="text/html" title="DeepSeek: A More Extreme Story of Chinese Tech Idealism"/><published>2024-12-27T00:00:00+00:00</published><updated>2024-12-27T00:00:00+00:00</updated><id>https://wonjae.kim/blog/2024/Deep-seek-interview</id><content type="html" xml:base="https://wonjae.kim/blog/2024/Deep-seek-interview/"><![CDATA[<p><strong>DeepSeek: 더욱 극단적인 중국 기술적 이상주의 이야기</strong><br/> <em>원문: 2024년 7월 17일 09:01 <a href="https://mp.weixin.qq.com/s/r9zZaEgqAa_lml_fOEZmjg">「暗涌Waves」</a></em></p> <table> <tbody> <tr> <td>글</td> <td>위리리(于丽丽)</td> </tr> <tr> <td>편집</td> <td>류징(刘旌)</td> </tr> </tbody> </table> <hr/> <p>중국의 7개 대형 모델(大模型) 스타트업 가운데, DeepSeek(深度求索)은 가장 조용히 활동해왔지만, 매번 의외의 방식으로 사람들의 뇌리에 강렬히 각인된다.<br/> 1년 전, DeepSeek이 화제가 된 것은 그 뒤에 있는 양적(量化) 사모펀드 거인 ‘환팡(幻方)’ 때문이었다. 대형 IT 기업 이외에서 유일하게 A100 칩을 만 장이나 보유한 곳이었기 때문이다. 그리고 1년 뒤, DeepSeek은 사실상 중국 대형 모델 가격 경쟁을 야기한 발화점으로 다시금 스포트라이트를 받고 있다.</p> <p>AI 관련 이슈가 끊임없이 터져 나온 5월, DeepSeek은 단숨에 이름을 널리 알렸다. 계기는 “DeepSeek V2”라는 오픈소스 모델을 발표한 것이었는데, 이 모델은 전례 없는 가성비를 제시했다. 추론 비용(추론 시 소요되는 비용)을 100만 토큰당 단돈 1위안(한화 약 180원 수준)으로 낮춘 것이다. 이는 Llama3 70B 모델의 약 7분의 1이고, GPT-4 Turbo의 약 70분의 1에 해당한다.</p> <p>DeepSeek이 단숨에 ‘AI계의 삔둬둬(拼多多, 초저가 이커머스 플랫폼)’라는 별칭을 얻게 되자, 바이트댄스(字节), 텐센트(腾讯), 바이두(百度), 알리바바(阿里) 등 빅테크 기업들 역시 가격을 인하하기 시작했다. 이렇게 중국 대형 모델 간의 가격 전쟁의 막이 올랐다.</p> <p>하지만 이러한 격렬한 분위기는 한 가지 사실을 감추고 있다. 바로, 많은 대형 기술 기업들이 적자를 감수하며 보조금 형태로 가격을 낮추는 것과 달리, DeepSeek은 이윤이 난다는 점이다.</p> <p>그 배경에는 DeepSeek이 모델 구조 자체에서 전방위적 혁신을 꾀했다는 점이 있다. DeepSeek이 제시한 새로운 MLA(Multi-Head Latent Attention, 다중 헤드 잠재 주의 기제) 구조는 기존에 가장 흔히 사용되었던 MHA(Multi-Head Attention) 구조의 5%~13% 정도만 GPU 메모리를 점유한다. 또한 DeepSeek이 독자적으로 제안한 ‘DeepSeekMoESparse’ 구조는 연산량을 극도로 낮춰, 최종적으로 비용 절감을 이뤄냈다.</p> <p>실리콘밸리에서는 DeepSeek을 “동방에서 온 신비로운 힘”이라고 부른다. 시장조사기관 SemiAnalysis의 수석 애널리스트는 “DeepSeek V2 논문이 올해 최고 수준의 논문일 수 있다”고 평했다. OpenAI 전 직원 앤드루 카(Andrew Carr)는 논문이 “놀라운 통찰로 가득 차 있다”며, 자신의 모델에 논문에서 제시한 훈련 설정을 일부 적용해봤다고 한다. OpenAI 전 정책 총괄이자 Anthropic 공동 창업자인 잭 클라크(Jack Clark)는 “DeepSeek은 심오하고 난해한 인재들을 고용했다”며, 드론이나 전기차처럼 중국이 만든 대형 모델도 “무시할 수 없는 세력이 될 것”이라 평했다.</p> <p>실리콘밸리가 주도해온 AI 물결에서, 중국 회사가 이처럼 주목받는 일은 드물다. 여러 업계 인사들은 “모델의 구조적 혁신”이 전 세계 오픈소스 기초 모델 분야에서도 사례가 거의 없었기에 이 같은 반향이 일어났다고 말한다. 어느 AI 연구자는 “어텐션(Attention) 구조가 제안된 이후 수년간 크게 바뀐 적이 거의 없는데, 거기에 성공적으로 손을 대는 것은 드문 일”이라며, “결정 단계에서부터 대부분은 확신 부족으로 실행조차 못 하는 경우가 많다”고 평했다.</p> <p>한편, 지금까지 중국의 대형 모델 업체가 모델 구조 자체에서 혁신을 시도한 사례가 적었던 것은, 중국이 0에서 1을 창조하는 기술 혁신보다는 1에서 10으로 확장하는 응용 혁신에 더 능숙하다는 편견, 그리고 무경로 상태에서 혁신하는 것은 위험 부담이 크고 확실한 성공을 담보하기 어렵다는 인식도 작용했기 때문이다. 무엇보다도, 이미 나온 최신 모델 구조를 빠르게 따라가고, 실제 적용에만 집중해도 된다는 분위기가 있었다. 모델 구조부터 혁신해 보겠다는 것은 사전에 선례가 없고 수많은 실패를 감수해야 하며, 시간과 비용이 막대하게 든다.</p> <p>그러나 DeepSeek은 오히려 역행을 택했다. 모든 이가 “대형 모델 기술은 어차피 수렴할 것”, “이미 나온 구조를 따라가는 편이 똑똑한 지름길”이라고 말하는 가운데, DeepSeek은 시행착오에서 얻을 수 있는 가치를 중시했다. 또한 중국의 대형 모델 스타트업도 기존 기술을 응용하는 것 말고, 글로벌 기술 혁신의 물결에 동참할 수 있다고 믿는다.</p> <p>DeepSeek의 결정 방식은 곳곳에서 다르다. 지금까지 중국의 7개 대형 모델 스타트업 중 유일하게, “모델도 하고, 애플리케이션도 한다”는 ‘둘 다 잡기’ 노선을 포기하고 연구·기술만 집중해온 곳이 DeepSeek이다. 직접 소비자(To C)용 제품을 전혀 내놓지 않았고, 전방위적 상업화를 고려하기보다는 오픈소스와 공개 노선을 고수해왔으며, 심지어 단 한 번도 투자를 유치한 적이 없다. 그 결과, 대부분의 사람들 시야에서 자주 잊히지만, 커뮤니티에서는 유저들이 ‘자발적 홍보(自来水)’ 방식으로 DeepSeek을 퍼뜨리곤 한다.</p> <p>DeepSeek은 어떻게 탄생했을까? 이를 위해 좀처럼 모습을 드러내지 않는 DeepSeek의 창업자 량원펑(梁文锋)과 인터뷰를 진행했다.</p> <p>환팡(幻方) 시절부터 기술 연구에만 몰두해온 80년대생 창업자 량원펑은, DeepSeek에 와서도 여전히 매일 “논문 읽고, 코드 짜고, 소그룹 토론에 참여하는” 일상을 연구원들과 함께한다.</p> <p>다수 업계 관계자와 DeepSeek 연구원은 “량원펑은 현재 중국 AI 업계에서 보기 드문, 인프라(Infra) 레벨의 공학 역량과 모델 연구 역량을 모두 갖추고, 필요한 자원까지 효율적으로 움직일 수 있는 사람”이라고 소개한다. “높은 차원에서 정확한 판단을 내릴 수 있으면서도 디테일 측면에서는 현장 연구원보다도 뛰어나다”는 평가를 받고 있으며, “무시무시한 학습 능력”을 갖췄지만 “전혀 사장(老板) 같지 않고, 오히려 순수한 괴짜(Geek) 같다”고도 한다.</p> <p>이번 인터뷰는 특히나 귀한 기회다. 인터뷰 속, 이 기술적 이상주의자는 중국 기술계에서 현재 매우 드문 관점을 제시한다. “이익과 손해(利害)보다 옳고 그름(是非)을 우선시하고, 시대적 관성을 인식하며 ‘원천적 혁신’을 의제에 올려놓아야 한다고 강조”하는 사람이다.</p> <p>1년 전, DeepSeek이 막 시장에 뛰어들었을 때, 우리는 량원펑과 첫 인터뷰를 진행했다(「미친 환팡: 한 보이지 않는 AI 거인의 대형 모델 행보」). 당시 그가 말한 “반드시 미친 듯이 야심을 품고, 동시에 미친 듯이 진심을 지녀야 한다(务必要疯狂地怀抱雄心，且还要疯狂地真诚)”는 문장은 말 그대로 멋진 구호처럼 들렸다. 그런데 1년이 지난 지금, 그것이 점점 행동으로 옮겨지는 중이다.</p> <hr/> <h2 id="인터뷰-전문">인터뷰 전문</h2> <h3 id="가격-전쟁의-첫-포탄은-어떻게-터졌나">가격 전쟁의 첫 포탄은 어떻게 터졌나?</h3> <p><strong>「暗涌」</strong>: DeepSeek V2 모델을 발표한 뒤 곧바로 대형 모델 간 가격 전쟁이 일어났습니다. 어떤 분들은 DeepSeek이 업계의 ‘메기(鲶鱼)’ 역할을 했다고 하죠.<br/> <strong>량원펑</strong>: 저희가 의도적으로 메기가 되려고 한 건 아니고, 어쩌다 보니 그렇게 되었습니다.</p> <p><strong>「暗涌」</strong>: 이런 결과는 예상했나요?<br/> <strong>량원펑</strong>: 매우 의외였습니다. 가격 문제에 사람들이 이렇게 예민할 줄 몰랐어요. 저희는 그저 저희 페이스대로 하던 일을 했고, 비용을 계산해 적정 가격을 책정했을 뿐입니다. 저희 원칙은 적자를 보지 않되, 폭리를 취하지도 않는 것이었어요. 그래서 원가 위에 약간의 이윤만 얹은 가격이었죠.</p> <p><strong>「暗涌」</strong>: 모델 발표 5일 뒤 즈푸AI(智谱AI)가 따라 내렸고, 이어 바이투댄스(字节), 알리(阿里), 바이두(百度), 텐센트(腾讯) 등 대형 기업들이 줄줄이 가격을 내렸습니다.<br/> <strong>량원펑</strong>: 즈푸AI가 내린 것은 입문급 제품이었어요. 저희와 같은 급의 모델은 여전히 비싸게 받더군요. 바이트댄스가 사실상 가장 먼저 뒤따랐는데, 플래그십 모델 가격을 저희와 동일한 수준으로 내렸습니다. 그러자 다른 빅테크들도 우르르 내리기 시작했어요. 그런데 대기업들은 우리보다 모델 비용이 훨씬 높을 텐데도, 적자를 감수하고 이를 진행하더군요. 결국 인터넷 시대의 ‘보조금 살포형 가격 전쟁’ 로직으로 전개된 거죠.</p> <p><strong>「暗涌」</strong>: 겉으로 보면 가격 인하는 사용자를 유치하기 위한 것 같은데, 인터넷 시대 가격 전쟁이 보통 그런 식이잖아요.<br/> <strong>량원펑</strong>: 사실 저희의 주된 목적은 사용자를 빼앗으려는 것이 아닙니다. 저희가 가격을 낮춘 건, 우선 다음 세대 모델 구조에 대한 연구를 통해 비용이 실제로 낮아졌기 때문이고, 또 API든 AI든 대중이 널리 싸게 쓸 수 있어야 한다고 생각했기 때문입니다.</p> <p><strong>「暗涌」</strong>: 보통 중국 업체들은 이번 세대 Llama 구조를 그대로 가져다 애플리케이션을 만드는데, 왜 DeepSeek은 모델 구조부터 건드렸나요?<br/> <strong>량원펑</strong>: 목표가 애플리케이션이라면 Llama 구조를 그대로 쓰면서 빠르게 제품을 내는 것이 당연히 합리적일 수 있습니다. 하지만 저희 목표는 AGI입니다. 이는 한정된 자원으로 더 강력한 모델을 구현하려면 새로운 모델 구조를 연구해야 함을 의미하죠. 대규모로 모델을 확장(scale up)하려면, 이런 기초 연구가 필요합니다. 구조뿐 아니라, 데이터를 어떻게 구성하고 어떻게 모델을 인간처럼 만들 것인지 등, 매우 다각적인 시도를 하고 있고, 이를 자사 모델에 구현해놓았습니다. 또한 Llama 구조는 훈련 효율이나 추론 비용 면에서 해외 최신 모델과 비교했을 때 2세대 정도 뒤처졌다고 생각합니다.</p> <p><strong>「暗涌」</strong>: 그 ‘2세대 격차’는 주로 어디서 나오나요?<br/> <strong>량원펑</strong>: 첫째는 훈련 효율 차이입니다. 국내 최고 수준과 해외 최고 수준을 견주면, 모델 구조와 학습 역학(Training Dynamics)에서 대략 2배 차이가 난다고 보는데, 결국 같은 성능을 내려면 우리는 2배의 컴퓨팅 자원을 써야 한다는 뜻이 됩니다. 둘째는 데이터 효율성에서도 1배 차이가 날 수 있는데, 같은 효과를 내기 위해선 2배의 훈련 데이터와 계산량이 필요한 것이죠. 이 둘을 합치면 4배의 계산 자원이 필요한 셈입니다. 저희는 이 격차를 줄이는 데 매진하고 있습니다.</p> <p><strong>「暗涌」</strong>: 대부분 중국 회사들이 모델과 애플리케이션 양쪽을 다 하는 ‘둘 다 잡기’를 선택하는데, DeepSeek은 왜 연구·탐색만 하고 있나요?<br/> <strong>량원펑</strong>: 저희는 지금 단계에서 글로벌 혁신의 흐름에 참여하는 게 가장 중요하다고 생각합니다. 과거엔 누군가 기술 혁신을 하면 우리가 가져다 적용해 돈을 버는 형태에 익숙했는데, 사실 그건 당연한 이치가 아니죠. 이번 물결에 저희는 “이 기회에 돈을 왕창 벌자”가 아니라, “기술 전선의 최전방으로 뛰어들어 생태계를 함께 발전시켜보자”는 생각을 갖고 있습니다.</p> <p><strong>「暗涌」</strong>: 인터넷·모바일 인터넷 시대를 거치면서, “미국은 0에서 1로 만드는 기술 혁신에 능하고, 중국은 1에서 10으로 확장하는 데 강하다”는 인식이 굳어졌습니다.<br/> <strong>량원펑</strong>: 중국 경제가 성장함에 따라, 중국도 이제는 단순히 ‘무임승차(搭便车)’만 할 수는 없습니다. 지난 30여 년간의 IT 물결에서 중국은 진정한 의미의 기술 혁신에 거의 참여하지 못했습니다. ‘모어의 법칙’(무어의 법칙)이 마치 하늘에서 떨어지는 선물처럼, 18개월마다 자동으로 더 나은 하드웨어와 소프트웨어가 나오는 것처럼 여긴 것이죠. “스케일링 법칙(Scaling Law)”도 마찬가지 취급을 받습니다.<br/> 하지만 사실, 이는 서구가 주도해온 기술 커뮤니티가 대대로 끊임없이 만들어낸 결실입니다. 우리가 그 과정에 참여하지 못했기 때문에 이를 간과해온 측면이 큰 거죠.</p> <h3 id="진짜-격차는-1년-혹은-2년이-아니라-원천적-혁신과-모방-사이에-있다">진짜 격차는 1년 혹은 2년이 아니라, ‘원천적 혁신’과 ‘모방’ 사이에 있다</h3> <p><strong>「暗涌」</strong>: 왜 DeepSeek V2에 실리콘밸리가 놀랐을까요?<br/> <strong>량원펑</strong>: 미국에서는 매일 엄청난 양의 혁신이 일어나고 있어서, 이 또한 그 가운데 하나일 뿐입니다. 그들이 놀란 것은 이 혁신을 ‘중국 회사’가 ‘혁신 기여자’로서 그들의 게임에 참여했다는 점 때문이죠. 대부분 중국 회사들은 혁신보다는 팔로우(Follow)를 택하니까요.</p> <p><strong>「暗涌」</strong>: 중국 문맥에서 보면 그런 선택이 너무 사치스럽게 느껴지기도 합니다. 대형 모델은 자본 투자가 크게 필요한 분야인데, 당장 돈 벌 수 있는 상업적 방안을 우선시하기 마련이잖아요.<br/> <strong>량원펑</strong>: 혁신에 드는 비용이 결코 적지 않다는 것은 사실입니다. 예전의 ‘가져다 쓰는’(拿来主义) 방식도 과거 국내 사정이 그럴 만했기 때문이죠. 그러나 지금은, 중국의 경제 규모나 바이트댄스·텐센트 같은 대기업의 이익을 볼 때, 글로벌에서 손꼽히는 수준입니다. 혁신에 돈이 없는 게 아니라, 확신이 부족하고 ‘고밀도의 인재 풀을 어떻게 효과적으로 조직해 제대로 된 혁신을 만들어낼지’를 모르고 있을 뿐입니다.</p> <p><strong>「暗涌」</strong>: 그래서 자금이 넉넉한 대기업조차도 빠른 상업화에 목을 매는 분위기가 되었군요.<br/> <strong>량원펑</strong>: 지난 30년간 우리는 돈 버는 일에만 집중했습니다. 기술 혁신은 외면받았어요. 하지만 혁신은 단순히 사업적 동기만으로는 부족하고, 호기심과 창조욕도 있어야 합니다. 다만 우리 사회가 그간 돈을 벌어야 한다는 관성에 매여 있었을 뿐이고, 그것도 하나의 과도기적 현상이라 볼 수 있겠지요.</p> <p><strong>「暗涌」</strong>: 하지만 결국 DeepSeek도 기업이지, 공익 연구소가 아니잖아요. 혁신한 다음 그것을 그대로 오픈소스해서 공개해버린다면, 과연 어떤 ‘해자(护城河, 경쟁적 진입 장벽)’를 만들 수 있을까요? 이번 5월에 발표한 MLA 구조 혁신도 곧 남들이 복제할 것 같은데요.<br/> <strong>량원펑</strong>: 혁신적인 기술은 닫아둬도 오래가진 못합니다. OpenAI도 비공개(Closed)로 하긴 하지만, 결국 다른 이들이 못 따라오게 막을 수는 없죠. 그래서 저희는 사람(팀)에 가치를 축적합니다. 우리의 동료들은 이런 과정을 거치며 성장하고 많은 노하우를 쌓고, 창의적 문화를 형성하죠. 이것이 곧 우리의 해자가 됩니다.<br/> 오픈소스와 논문 발표로 저희가 잃는 것은 사실상 별로 없어요. 기술자 입장에서는 남들이 우리를 따라오면 큰 보람을 느낍니다. 사실 오픈소스는 일종의 문화적 행동이라고 볼 수 있어요. 기업 차원에서 이렇게 공유하는 것은, ‘추가적인 명예’를 얻는 것이기도 합니다. 회사 입장에서도 문화적 매력을 갖게 되죠.</p> <p><strong>「暗涌」</strong>: “대형 모델은 결국 시장이 승부를 가른다”고 주장하는, 예컨대 주샤오후(朱啸虎) 같은 투자자의 의견에 대해서는 어떻게 생각하시나요?<br/> <strong>량원펑</strong>: 주샤오후는 자기 논리 체계 내에서 자족적인 분입니다. 그분의 방식은 ‘빠른 돈’을 노리는 회사에 더 맞을 수 있습니다. 하지만 미국에서 가장 돈을 잘 버는 회사들은 두껍게 쌓인 기술(厚积薄发)로 성장한 하이테크 기업들이죠.</p> <p><strong>「暗涌」</strong>: 대형 모델이라는 판에서, 기술적 우위만으로 절대적 우세를 지속하기 쉽지 않을 텐데, DeepSeek이 궁극적으로 노리는 건 뭔가요?<br/> <strong>량원펑</strong>: “중국 AI가 영원히 뒤따르기만 할 수는 없다”는 점입니다. 흔히 “중국 AI와 미국은 1~2년 차이가 있다”고 말하지만, 실제 격차는 ‘원천적 혁신과 모방’의 차이입니다. 이게 바뀌지 않는다면, 중국은 영원히 추격자일 뿐이죠. 언젠가는 우리가 부딪혀야 할 도전입니다.<br/> 엔비디아가 지금처럼 앞서게 된 것은 한 회사의 힘만이 아니라, 서구 기술 커뮤니티 전체, 산업 전반의 축적 덕택입니다. 그들은 차세대 기술 트렌드를 미리 보고, 로드맵을 갖고 있습니다. 중국도 똑같이 이런 생태계를 갖춰야 하죠. 그래서 누군가는 기술 최전방에 서 있어야 하고, 그다음 세대의 열쇠를 찾아와야 합니다.</p> <h3 id="더-많이-투자하면-더-많은-혁신이-나오는-건-아니다">“더 많이 투자하면 더 많은 혁신이 나오는 건 아니다”</h3> <p><strong>「暗涌」</strong>: 지금 DeepSeek은 OpenAI 초창기 같은 이상주의적 기풍이 느껴지고, 게다가 오픈소스이기도 하죠. 향후에 ‘비공개(Closed)’로 전환할 수도 있을까요? OpenAI나 미스트랄(Mistral)도 오픈소스에서 비공개로 전환한 경험이 있잖아요.<br/> <strong>량원펑</strong>: 저희는 비공개로 전환하지 않을 겁니다. 탄탄한 기술 생태계를 먼저 확보하는 게 더 중요하다고 보기 때문입니다.</p> <p><strong>「暗涌」</strong>: 투자 유치 계획은 없나요? 매체 기사 중에는, 환팡이 DeepSeek을 분리해 상장시키려 한다는 내용도 있던데요. 실리콘밸리 AI 스타트업들을 보면 결국 대기업과 제휴해 나가게 되잖아요.<br/> <strong>량원펑</strong>: 단기적으로는 투자 유치 계획이 없습니다. 저희가 겪는 문제는 단 한 번도 돈이 부족해서가 아니라, 고성능 칩(High-end GPU 등)을 금수당하는 문제였으니까요.</p> <p><strong>「暗涌」</strong>: 일각에서는 “AGI와 양적 트레이딩(量化)은 완전히 다른 종목이다. 양적 펀드는 조용히 할 수 있어도, AGI는 대대적인 진영 구축과 연대가 필요해 대규모 투자가 중요한데, DeepSeek은 그만큼 자원이 충분한가?”라고 의문을 제기하기도 합니다.<br/> <strong>량원펑</strong>: 투자나 자원이 많다고 반드시 더 많은 혁신이 나오진 않습니다. 그렇다면 대기업들이 모든 혁신을 다 독점했겠죠.</p> <p><strong>「暗涌」</strong>: 현재 애플리케이션을 안 하는 건, 운영 역량이 부족해서가 아닌가요?<br/> <strong>량원펑</strong>: 지금은 기술 혁신이 폭발하는 시기이지, 애플리케이션이 폭발하는 시기가 아니라고 봅니다. 저희는 장기적으로 생태계를 만드는 걸 지향합니다. 즉, 업계에서 저희 기술과 성과물을 직접 활용할 수 있게 하고, 저희는 기초 모델과 최첨단 혁신만 맡으며, 다른 회사가 DeepSeek을 기반으로 해서 To B, To C 영역의 사업을 전개하는 것이죠. 산업 가치사슬이 완비되어 있다면, 저희가 굳이 직접 애플리케이션을 만들 필요가 없습니다. 물론 필요하다면 직접 애플리케이션을 하는 것도 문제 없지만, 연구와 기술 혁신이 항상 최우선순위가 될 겁니다.</p> <p><strong>「暗涌」</strong>: 그렇다면 API 제공 업체로서, 왜 DeepSeek을 선택해야 하죠? 대기업의 API가 더 나을 수도 있지 않나요?<br/> <strong>량원펑</strong>: 미래 세계는 전문적 분업이 더 심화될 겁니다. 기초 대형 모델은 지속적인 혁신이 필요한데, 대기업이 모든 걸 책임지는 데 한계가 있을 수 있어요.</p> <p><strong>「暗涌」</strong>: 기술이 정말 큰 격차를 만들어낼 수 있을까요? 말씀하신 대로 절대적인 ‘기술 비밀’은 없다고 하셨는데요.<br/> <strong>량원펑</strong>: 기술은 비밀이 아니라도, 그걸 재현하려면 시간과 비용이 듭니다. 엔비디아 GPU에 대단한 기술 비밀이 있어서가 아니라, 새 팀을 꾸려 따라잡기엔 시차가 있고, 또 그 사이 엔비디아는 다음 세대를 준비하기 때문에 실제로 큰 해자가 생기는 거죠.</p> <p><strong>「暗涌」</strong>: 이번에 DeepSeek이 가격을 내린 후, 바이트댄스가 발 빠르게 따라 한 것은 그들이 위협을 느꼈기 때문일 겁니다. 대기업과 스타트업이 경쟁하는 새로운 해법을 어떻게 보나요?<br/> <strong>량원펑</strong>: 사실 저희는 그 문제에 신경을 많이 쓰지 않습니다. 저희 목표가 클라우드 서비스를 통해 이익을 내는 게 아니니까요. 우리의 목표는 AGI를 실현하는 것이며, 이번 가격 인하는 그 과정에서 ‘곁다리로’ 한 것입니다.<br/> 지금으로서는 새로운 해법이 눈에 띄지는 않습니다. 대기업들은 이미 사용자 기반을 갖고 있지만, 동시에 현금 흐름을 책임져야 하고 그것이 오히려 족쇄가 되어 언제든 뒤집힐 수 있다는 리스크가 있죠.</p> <p><strong>「暗涌」</strong>: DeepSeek 외에 다른 6개 대형 모델 스타트업들의 ‘최종 성적표’를 어떻게 전망하나요?<br/> <strong>량원펑</strong>: 아마 2~3곳은 살아남을 겁니다. 아직은 다들 돈을 계속 태우는 단계죠. 그래서 자기定位가 분명하고, 정교한 운영 역량이 있는 회사만 살아남을 가능성이 높습니다. 그렇지 않은 곳들도 어쩌면 탈태환골(脱胎换骨, 완전히 새롭게 거듭남)할 수 있어요. 가치 있는 것은 사라지지 않겠지만, 다른 형태가 될 수 있죠.</p> <p><strong>「暗涌」</strong>: 환팡 시절부터 경쟁에 대한 태도는 늘 “마이웨이”였다고 평가받았습니다. 경쟁을 바라보는 근본적인 시각이 뭔가요?<br/> <strong>량원펑</strong>: 저는 늘 “사회 전체의 효율을 높일 수 있는가, 그리고 그 가치사슬 속에서 내가 잘할 수 있는 자리를 찾을 수 있는가”를 생각합니다. 결과적으로 사회 효율이 높아지면, 그건 유의미하다고 봐요. 중간 과정에서 벌어지는 일들에 과도하게 매달리면 정신없어집니다.</p> <h3 id="깊고-난해한-일을-하는-젊은이들">“깊고 난해한 일을 하는” 젊은이들</h3> <p><strong>「暗涌」</strong>: OpenAI 전 정책 총괄이자 Anthropic 공동 창업자인 잭 클라크가 “DeepSeek은 ‘심오하고 난해한 기재(奇才)’들을 고용했다”고 언급했습니다. DeepSeek V2를 만든 사람들은 어떤 분들인가요?<br/> <strong>량원펑</strong>: 그렇게 ‘심오하고 난해한 천재’들이 아닙니다(웃음). 주로 톱급 대학 출신 신입사원, 박사 4~5년 차 실습생, 그리고 졸업한 지 몇 년 안 된 젊은이들이에요.</p> <p><strong>「暗涌」</strong>: 많은 대형 모델 회사들이 해외 인재를 적극 영입하는데, “이 분야 최상위 50명의 인재는 중국 회사에 거의 없을 것”이라는 인식이 있잖아요. 그런데 DeepSeek V2를 만든 팀은 전원 본토 출신이라고요?<br/> <strong>량원펑</strong>: 네, 해외에서 돌아온 팀원은 없었어요. 물론 최상위 50명 중 상당수가 해외에 있을 수 있지만, 저희는 “우리가 직접 그런 인재를 키워낼 수도 있지 않을까”라고 생각합니다.</p> <p><strong>「暗涌」</strong>: MLA 혁신은 어떻게 탄생한 건가요? 들리는 얘기로는, 한 젊은 연구원이 개인적 흥미로 시작했다고 하던데요.<br/> <strong>량원펑</strong>: 어텐션(Attention) 구조의 주류 변화 패턴들을 정리하던 중, 그는 대안을 설계해보고 싶다는 생각이 들었다고 해요. 하지만 아이디어가 실제로 구현되기까지는 꽤 오랜 시간이 걸렸습니다. 그 일을 위해 특별 팀을 꾸렸고, 몇 달간의 작업 끝에 결과를 냈어요.</p> <p><strong>「暗涌」</strong>: 이런 발산적 아이디어가 실현된 것은, ‘완전히 혁신 지향적인’ 여러분의 조직문화 덕분인 것 같습니다. 환팡 시절부터 상명하달식 과제 지시가 없었던 것으로 압니다. AGI 같은 불확실성이 큰 분야에서도 여전히 비슷한가요?<br/> <strong>량원펑</strong>: DeepSeek에서도 전적으로 ‘아래에서 위로(自下而上)’ 방식입니다. 특정 목표나 과제를 미리 던져주지 않아요. 각자는 이미 자기만의 경험과 아이디어가 있어서 누가 등을 떠밀지 않아도 알아서 시도합니다. 그러다가 문제에 부딪히면 주변 사람을 불러 토론하죠. 그런데 어떤 아이디어가 가능성을 보이면, 그때는 위에서 아래로(自上而下) 자원을 투입합니다.</p> <p><strong>「暗涌」</strong>: DeepSeek은 GPU 리소스와 인력을 매우 유연하게 쓴다고 들었어요.<br/> <strong>량원펑</strong>: 저희는 한 사람이 훈련 클러스터 GPU를 얼마나 쓸지 상한을 두지 않습니다. 아이디어만 있다면 누구나 승인 없이 카드(GPU)를 호출해 실험할 수 있어요. 또 조직에 계층이 없기 때문에, 필요한 사람이 있으면 누구든지 데려와 협업할 수 있습니다. 서로가 관심만 있다면 언제든 팀을 구성하죠.</p> <p><strong>「暗涌」</strong>: 그런 느슨한 방식이 작동하려면, 스스로 도전의식을 갖는 사람을 뽑아야 할 텐데요. 듣기로는 DeepSeek은 독특한 채용 방식을 쓴다고 하던데요. 일반적인 지표 외에 특이한 배경의 사람들도 많이 뽑는다고요.<br/> <strong>량원펑</strong>: 저희가 사람을 볼 때 가장 중시하는 것은 ‘열정과 호기심’입니다. 그래서 흔히 말하는 ‘독특한 이력’을 가진 사람이 꽤 있어요. 돈보다는 연구 자체를 갈망하는 분들이죠.</p> <p><strong>「暗涌」</strong>: 트랜스포머(Transformer)가 구글 AI 랩에서 탄생했고, ChatGPT가 OpenAI에서 탄생했는데, 이렇게 대기업 AI 연구소가 혁신의 산실이 되곤 하잖아요. 창업 스타트업과 대기업 AI 랩이 만들어내는 혁신 가치는 뭐가 다를까요?<br/> <strong>량원펑</strong>: 구글 랩, OpenAI, 그리고 중국 대기업 AI 랩도 모두 가치가 있습니다. 최종적으로 OpenAI가 ChatGPT를 만들어냈지만, 사실 거기엔 역사적 우연 요소가 있죠.</p> <p><strong>「暗涌」</strong>: 혁신은 어느 정도 우연이 작용한다는 뜻인가요? DeepSeek 사무실 중앙 회의실에 양옆으로 언제든 활짝 열 수 있는 문이 달려 있던데, 동료들 말로는 “우연을 위한 여유 공간”이라고 하더군요. 트랜스포머가 탄생하던 때도, 옆방을 지나다 끼어든 사람이 아이디어를 바꿨다는 얘기가 있잖아요.<br/> <strong>량원펑</strong>: 혁신은 우선 ‘믿음’이 있어야 합니다. 왜 실리콘밸리가 혁신 정신이 강하냐고 하면, 그들은 일단 “해볼 만하다!”고 과감하게 나서죠. ChatGPT가 등장했을 당시, 중국은 투자자나 대기업 모두 격차가 너무 크다며, “어차피 힘드니 애플리케이션이나 만들자”는 분위기가 대부분이었습니다. 그런데 혁신을 하려면 먼저 자신감이 필요합니다. 이 자신감은 오히려 젊은 층에게서 더 많이 볼 수 있어요.</p> <p><strong>「暗涌」</strong>: 그래도 투자를 안 받고, 대외 홍보도 거의 안 하면, 소셜 미디어에서의 존재감은 상대적으로 약하겠죠. 대형 모델을 하려는 인재가 DeepSeek을 1순위로 택할까요?<br/> <strong>량원펑</strong>: 저희는 “세상에서 제일 어려운 문제를 풀고 있다”고 자부합니다. 최고급 인재를 가장 끌어당기는 요인은, 세계에서 가장 어려운 문제를 해결해보려는 도전감입니다. 사실 중국에는 이런 ‘최고급 인재’가 꽤 많지만, 우리 사회 전반에 하드코어 혁신이 적으니 이들이 발굴되거나 주목받을 기회가 적었을 뿐이라고 생각합니다. 우리가 정말 어려운 문제에 도전하고 있으니, 그 자체가 큰 매력이 되죠.</p> <p><strong>「暗涌」</strong>: 얼마 전 OpenAI가 신제품을 내놨지만, GPT-5는 아니었습니다. 그래서 성장 곡선이 눈에 띄게 둔화되었다, 스케일링 법칙(Scaling Law)에 한계가 나타난다는 말이 나오는데요. 어떻게 보시나요?<br/> <strong>량원펑</strong>: 저희는 오히려 낙관적인 편입니다. 업계 전반이 예측대로 가고 있다고 봅니다. OpenAI라고 해서 신이 아니니, 계속 선두에만 있을 수는 없겠죠.</p> <p><strong>「暗涌」</strong>: AGI는 언제쯤 가능하다고 예상하시나요? DeepSeek은 V2 발표 전, 코드 생성 및 수학 모델 등을 발표했고, Dense 모델에서 MoE 모델로도 전환했습니다. 그 과정을 보면 귀사 AGI 로드맵의 핵심 좌표는 뭔가요?<br/> <strong>량원펑</strong>: 2년 후일 수도 있고, 5년 후나 10년 후일 수도 있습니다. 다만 우리가 살아 있는 동안에 실현될 거라고 봐요. 로드맵은 회사 내부에서도 통일된 의견이 없지만, 우선 세 가지 방향을 주목하고 있습니다. 첫째는 수학과 코드, 둘째는 멀티모달, 셋째는 자연언어 자체입니다. 수학과 코드는 AGI를 시험하기에 좋습니다. 바둑처럼 폐쇄적이고 검증 가능한 시스템이어서, 자가 학습을 통해 높은 수준의 지능 달성을 노려볼 수 있어요. 한편, 다중 모달로 인간의 현실 세계에 참여해 학습하는 것도 AGI 달성에 필수적일 수 있습니다. 우리는 모든 가능성에 열려 있습니다.</p> <p><strong>「暗涌」</strong>: 대형 모델의 ‘최종 상태’는 어떻게 될까요?<br/> <strong>량원펑</strong>: 기초 모델과 기초 서비스를 전문적으로 제공하는 회사들이 있을 것이고, 그 위에 긴 가치사슬을 따라 다양한 수요를 충족하는 회사들이 나타날 겁니다.</p> <h3 id="모든-정해진-방식은-이전-세대의-산물일-뿐이다">“모든 ‘정해진 방식’은 이전 세대의 산물일 뿐이다”</h3> <p><strong>「暗涌」</strong>: 지난 1년간 중국 대형 모델 스타트업 업계에도 많은 변화가 있었죠. 초창기에 활발히 나섰던 왕후이원(王慧文) 같은 인물도 중도 하차했고, 새로 뛰어든 회사들도 차별화를 시도하고 있습니다.<br/> <strong>량원펑</strong>: 왕후이원은 자기 돈으로 모든 손실을 감당했죠. 다른 사람들은 온전히 보전해주고요. 스스로에게 가장 손해가 큰 선택을 했지만, 모두에게 도움이 되는 결정을 했어요. 굉장히 인품이 훌륭한 분이고, 저도 존경합니다.</p> <p><strong>「暗涌」</strong>: 요즘 가장 많은 에너지를 어디에 쏟고 계신가요?<br/> <strong>량원펑</strong>: 차세대 대형 모델 연구가 대부분입니다. 아직 미해결 문제가 정말 많거든요.</p> <p><strong>「暗涌」</strong>: 다른 몇몇 대형 모델 스타트업들은 “기술 우위를 서둘러 애플리케이션에 적용해 시너지를 내야 한다”고 주장합니다. DeepSeek이 아직 모델 연구에만 집중하는 것은, 모델 역량이 충분하지 않기 때문인가요?<br/> <strong>량원펑</strong>: “모든 ‘정해진 방식’은 이전 세대의 산물”이고, 미래에는 통할지 알 수 없습니다. 인터넷 시대의 사업 논리로 AI 시대의 수익 모델을 논한다면, 마치 마화텅(马化腾, 텐센트 창업자)이 처음 창업할 때 “제너럴 일렉트릭과 코카콜라의 사례를 가지고 미래를 예측”하는 것과 비슷하죠. 그건 일종의 ‘각주구검(刻舟求剑)’일 수 있습니다.</p> <p><strong>「暗涌」</strong>: 환팡은 강력한 기술·혁신 DNA를 바탕으로 꽤 순조롭게 성장한 편인데, 그 기억 때문에 낙관하시는 건가요?<br/> <strong>량원펑</strong>: 환팡 시절의 경험은 기술 혁신으로 큰 성과를 낼 수 있다는 자신감을 심어줬습니다. 그러나 그 길이 꼭 평탄하기만 했던 건 아니에요. 환팡이 2015년 이후 모습을 드러냈지만, 사실 저희는 16년간 쌓아왔습니다.</p> <p><strong>「暗涌」</strong>: 다시 ‘원천적 혁신’ 주제로 돌아가면, 지금 경제가 내리막에 접어들고 자본 시장도 냉랭해졌습니다. 이것이 오히려 혁신을 더 억제하지 않을까요?<br/> <strong>량원펑</strong>: 저는 꼭 그렇지는 않을 것 같아요. 중국 산업 구조가 바뀌려면, 결국 하드코어 기술 혁신이 더 중요해집니다. 과거에는 누구나 ‘쉽게 돈 버는’ 분위기가 있었는데, 실제로 그게 ‘시대적 운’에서 비롯되었음을 자각하게 된다면, 앞으로는 정말 무엇인가를 만들어내는 혁신에 더 매달리게 될 겁니다.</p> <p><strong>「暗涌」</strong>: 이 부분에서는 낙관적이시군요.<br/> <strong>량원펑</strong>: 저는 80년대에 광둥성(广东) 5선 도시에서 자랐고, 아버지는 초등학교 교사였습니다. 90년대 광둥에는 돈 벌 기회가 넘쳤는데, 주변 학부모들이 제게 “공부해봐야 뭐하냐”며 무시하곤 했어요. 그런데 지금 돌아보면 상황이 달라졌습니다. 돈이 예전처럼 쉽게 벌리지도 않고, 택시를 몰 기회조차도 여의치 않다는 말을 하죠. 한 세대 만에 시각이 바뀐 겁니다.<br/> 앞으로 하드코어 혁신이 점점 많아질 거고, 지금은 잘 이해받지 못하더라도 실제 성공 사례가 나오면 사회 전체가 한 번 더 학습하게 됩니다. 결국 집단적 인식도 바뀔 겁니다. 저희는 그저 ‘사실(fact)’들과 그것을 만들 시간, 과정이 더 필요할 뿐이에요.</p> <hr/> <table> <tbody> <tr> <td>이미지 출처</td> <td>IC Photo</td> </tr> <tr> <td>편집·디자인</td> <td>야오난(姚楠)</td> </tr> </tbody> </table> <hr/>]]></content><author><name></name></author><summary type="html"><![CDATA[DeepSeek 창업자 인터뷰 번역: 중국 기술적 이상주의 이야기]]></summary></entry><entry><title type="html">Exploiting Contemporary ML</title><link href="https://wonjae.kim/blog/2021/Exploiting_Contemporary_ML/" rel="alternate" type="text/html" title="Exploiting Contemporary ML"/><published>2021-01-02T00:00:00+00:00</published><updated>2021-01-02T00:00:00+00:00</updated><id>https://wonjae.kim/blog/2021/Exploiting_Contemporary_ML</id><content type="html" xml:base="https://wonjae.kim/blog/2021/Exploiting_Contemporary_ML/"><![CDATA[<p>이 글은 제가 졸업한 <a href="http://hcil.snu.ac.kr/">HCI 연구실</a>로부터 최신 ML 모델을 HCI 연구에 <strong>응용</strong>만 하길 원한다면 뭘 배워야 하는지 알려달라는 요청이 와서 간단한 기본 지식과 응용하고 싶은 분야를 묻는 설문지를 돌린다음 반응을 보고 적은 가이드라인입니다.</p> <h4 id="제목의-의미">제목의 의미</h4> <ul> <li>Exploiting: ML 모델에 대한 <strong>원론적 이해</strong>는 비전공자로서 모델 사용을 함에 있어 건너뛰어도 상관 없다는 뜻을 담았습니다.</li> <li>Contemporary: 어제 아카이브에 올라온 논문도 exploit 할 수 있어야 한다는 뜻을 담았습니다.</li> </ul> <h3 id="학교-강의의-한계">학교 강의의 한계</h3> <p>학교 수업에서 커버할 수 있는 지식은 어느 선까지로 정의해야 할까요? 어떤 학문이 가지를 쳐서 하나의 나무를 이룬다고 생각했을 때, 모든 가지가 공유하는 밑동에 해당하는 지식을 커버해야하는 것이 옳을 겁니다. 머신러닝이라는 학문에서 모든 가지가 공유하는 밑동은 뭘까요? 대부분의 학교 강의자는 이게 실해석학과 그 응용인 확률론이라는 것에 동의할 겁니다. 물론 맞는 말이지만, 너무 밑동입니다 (머신러닝 &lt; 통계학 &lt; 확률론 &lt; 실해석학 &lt; 해석학 &lt; 수학). 반대로 머신러닝의 가지인 세부 분야를 관찰해봅시다. 예컨대 수많은 음악 데이터를 기반으로 새로운 음악을 만들어내고 (더 정확하게는 샘플링하고) 싶다고 합시다. 중요한 부분만 짚으면 해당 분야의 위계는 아마 이럴 겁니다: 음악 생성 &lt; 소리 생성 &lt; 시계열 생성 &lt; 시퀀스 생성 &lt; 생성 모델 &lt; 머신러닝. 더 응용성이 높은 가지일수록 더 밑동의 지식과는 멀어지게 됩니다. 그 밑동의 지식을 전부 알아야 할 필요도 없어지게 됩니다.</p> <p>예를 들어 왜 시퀀스 생성 &lt; 생성 모델이냐 하면, 생성 모델의 종류로 많이 사용되는 방법만 보더라도 non-tractable distribution을 사용하는 방법, tractable 한 variational distribution을 사용하는 방법, 또 그 distribution의 값을 어떻게 data와 맵핑하는지에서 invertible한 방법 (volume-preserving), density를 보존하지 않는 방법 등 다양한 접근 방법이 있고 이들 중 <strong>시퀀셜 데이터</strong>에 잘 들어맞는 방법들이 따로 있고 <strong>논시퀀셜 데이터</strong>에 잘 들어맞는 방법들이 따로 있습니다. 물론 어떤 방법이 어떤 세부 태스크에 맞는지는 많은 empirical study가 이루어졌기 때문에 알 수 있는 부분이고요. 시퀀스 생성에서 현재 대세를 이루고 있는 생성 모델이 <em>volume-preserving mapping from diagonal gaussian distribution</em> (e.g., normalizing flows)이라고 가정해봅시다. 그럼 사실 음악 생성, 소리 생성, 시계열 생성을 응용하고자 하는 사람은 normalizing flows만 알면 됩니다. 만약 그 대세를 거슬러서 (예를 들었을 뿐이지 둘 다 대세입니다. 진짜 대세가 아닌 비주류는 오디오 처리를 주력으로 하지 않는 제가 몰라서 못적기 때문에 이렇게 두개를 적어놓은겁니다.) 다른 타입의 생성 모델로도, 예컨대 <em>free-form mapping from gaussian distribution; learning mapping via adversarial loss</em> (e.g., GANs)으로 normalizing flows의 성능을 이기고자 한다면 그 외 다른 생성 모델에 대해서도 잘 알아야겠지만 <strong>현재 SoTA를 응용만 하고자 하는 사람</strong>은 다시 말하지만 굳이 알 필요가 없습니다.</p> <p>하물며 생성 모델 또한 수 많은 머신러닝 모델 방법론중 (대세긴 하지만) 하나일 뿐인데, 우리가 <strong>머신러닝</strong>이라는 이름의 강의 전체를 듣는다 한들 얼마나 응용하고자 하는 부분이랑 겹치는 내용을 얻어갈 수 있을까요? 바로 어제 나온 SoTA 모델을 읽는다고 했을 때, 학교 강의에서 배운 걸 그대로 사용하는 부분은 거의 없을겁니다. 제가 말한 <strong>머신러닝</strong>이라는 이름의 강의의 대표격으로 <a href="https://www.coursera.org/learn/machine-learning">Andrew Ng의 강의</a>나 <a href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020">Murphy의 책</a>을 예로 들고 싶습니다. 이러한 명강의나 명저를 수십시간을 들여서 학습한다고 해도, 어제 나온 SoTA 논문을 읽고 구현할 수 없을 겁니다. 위에서 말했듯 이런 강의는 대세를 넘어서 새로운 아이디어로 새 논문을 쓸 연구자의 <strong>기초</strong> 코스로 훌륭할 뿐입니다. 머신러닝 연구를 위해서는 위와 같은 기초 (또 그걸 넘어서 실해석학과 미분기하학에 이르는)를 쌓고 연구하고자 하는 좁은 범위를 구성하는 어제까지 나온 굵직한 연구들의 literature를 알아야 합니다.</p> <p>그럼 우리는 EXPLOIT을 할거니까 저런 강의 말고 뭘 들어야 할 것인가? 4년 전까지는 <a href="http://cs231n.stanford.edu/">CS231n</a> 정도면 최신 연구를 읽고 구현하는데에 (즉, exploit 하는데에) 무리가 없었습니다. 물론 그 당시에도 CS231n만 들어서는; 다시 말하지만 새로운 방법론을 제시할만한 지식을 쌓기는 어려웠습니다. 그랬었기 때문에 CS231n을 롤모델로 많은 대학의 대학원 머신러닝 응용 강의나 학부 고학년 머신러닝 응용 강의가 개설되었고 지금도 유지되고 있습니다. 그런데 아쉽게도 CS231n이 최초 개설된 Winter 2015 이후로 머신러닝에는 2012~2015보다 더 큰 발전이 있었고 이는 들어가는 연구 인력과 자원의 양이 기하급수적으로 커졌으니 당연합니다. 이제는 이 강의만으로는 CS231n이 주력으로 삼는 컴퓨터 비전 영역이라도 어제 나온 논문을 읽고 구현하기 어렵게 되었습니다. 이게 적어도 앞으로 최근 연구를 반영한 강의가 나오기 전까지는 학교 강의가 가지는 한계입니다.</p> <h3 id="2016년-이후의-세계의-변화점">2016년 이후의 세계의 변화점<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></h3> <p>2016년 DeepMind의 AlphaGo Lee이후 구글은 본격적으로 머신러닝 기업으로의 전환을 모색했습니다. 머신러닝 기업이 가질 수 있는 배타성이 뭐가 있을까요? 현재 머신러닝 학계는 논문의 재현성에 엄청난 피어프레셔를 줍니다. 즉 논문을 사용하는데 들어간 데이터와 소스코드의 완전한 공개를 요구한다는 것이죠. 그럼 “이 분야에서 1등을 해봤자 우리 기업의 우수한 모델을 세계에 공유하게 되는데 어떡하지?” 라는 생각을 할 수밖에 없었을 겁니다. 가장 단순한 아이디어는 <strong>학술용 데이터셋</strong>과 별개로 <strong>사내 데이터셋</strong>을 구축하는 것입니다. 누구나 생각할 수 있고, 이미 <em>데이터 자본(주의)</em>라는 키워드로 많은 사람들이 알고있는 방법입니다. 실제로 구글과 페이스북은 각자 가지고있는 거대 데이터 파이프라인을 통해 JFT-300M이나 Instagram-1B 같은 비공개 데이터셋을 구축했습니다. 물론 학계에 논문을 낼때는 ImageNet 같은 (작은) 학술 데이터셋에 대한 리포트를 필수적 주요하게 첨부합니다.</p> <p>그럼 이 <em>데이터 자본</em>을 가지고 실제로 배타적인 강력한 모델을 만들어야 하는데요. 이 지점에서 기업들을 커다란 장애물을 하나 치워야 함을 느낍니다. 바로 convolution과 recurrence입니다. 혹시 이 두 구조가 딥러닝을 받드는 축이라고 생각하시나요? 아쉽게도 이제는 아닙니다. 두 구조는 (소규모 자본을 가진 입장에서는) 좋은 특징인 data efficient하다는 특징을 가지는데요. 말 그대로 적은 데이터에서도 overfitting이 다른 모델보다 적다는 뜻입니다. 이러한 특징은 데이터의 성질에 적당히 들어맞는 사람의 inductive bias를 모델에 주입해주면 갖춰집니다. 예로, convolution은 “어떤 값의 spatially close한 값들은 연관된 값이다” 라던지 말이죠; 실제로 이미지 데이터의 성질과 어느정도 일치하죠? <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>논문 <a href="https://arxiv.org/abs/1806.01261">Relational inductive biases, deep learning, and graph networks</a>에서 이러한 (relational) inductive bias (IB)의 분류를 정리합니다. 그리고 예시로 FC, Convolutional, Recurrent, Graph 4가지의 자주 사용되는 NN layer가 어떠한 IB를 가지는지 제시하는데요. 인풋으로 받는 데이터의 형태를 flatten해서 (L (length), C (channel))라고 할 때, 위 논문에서 다루는 relative IB는 L개의 C-dim vector 간의 relation에 어떠한 bias를 주입하는지에 대한 겁니다. 앞서 말했듯 convolution의 경우 spatially close = local한 vector간에 bidirection link를 조성하고, recurrent의 경우 local한 vector간의 unidirection link를 조성합니다. FC의 경우에는 어떠한 링크도 조성하지 않으므로 IB가 없다고 해석합니다. Graph의 경우 모든 vector간에 bidirection link를 조성하므로 가장 높은 IB가 가해져있다고 볼 수 있습니다.</p> <p>해당 논문에서는 <a href="https://psycnet.apa.org/record/1994-98068-004">The interaction of nature and nurture in development: A parallel distributed processing perspective</a>를 인용하여 높은 IB가 overfit을 방지한다고 말합니다. 하지만 반대로 생각하면 IB를 적게 주입하면 overfit이 될 가능성이 높은, 빠르게 수렴하는 모델을 확보할 수 있습니다. Convolution과 recurrence 구조는 이런 trade-off에서 적절한 IB 주입의 균형점을 찾았고 data efficient하다는 특징을 가질 수 있었습니다. FC와 graph 사이에는 convolution과 recurrent 말고도 사람이 직접 디자인한 도메인에 아주 잘 맞는 IB도 존재할 수 있습니다. 실제로 수 년간 이러한 “모델의 구조에 IB를 녹여볼까” 하는 연구는 딥러닝 연구의 주축이 되어왔습니다.</p> <p>하지만 엄청난 양의 데이터를 가진 기업은 최고 수준의 regularization을 필요로 하고, 결국 모든 vector들이 전부 연결된 임의 연결성 IB를 주입한 모델을 택할 수 밖에 없습니다. 그 결과 나온 모델이 2017년 NIPS에서 발표한 <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>의 Transformer입니다. Transformer와 Graph NN은 <a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">동치 관계</a>이고, 모든 vector들 간의 연결이 IB로 주입되어있습니다.</p> <p>높은 IB를 가진 모델을 fit 시키려면 많은 양의 데이터가 필요합니다. 즉, data efficient하지 않습니다. 실제로 “Attention is All You Need”에서도 데이터의 양이 적은 경우 기존의 recurrent 방법론과 대등하거나 안좋은 성능을 보입니다. 대중들이 논문의 수치적 결과만 보고 ‘attention is all you need 라더니 성능은 그냥 RNN하고 별 차이 없네’ 라고 생각하던 2018년 전반기, Allen AI에서 2018년 2월에 <a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a> (ELMo) 라는 논문을 <strong>LSTM</strong> 구조 기반으로 내놓습니다. ELMo는 pretrain-finetune 파이프라인을 성공적으로 학계에 제시했고, 연이어서 2018년 4월 <a href="https://arxiv.org/abs/1804.07461">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a>, GLUE 벤치마크가 공개됩니다. 이 논문에서 baseline으로 사용된 ELMo는 또 뛰어난 성능을 보였고, pretrain-finetune 파이프라인의 성능을 다시 입증합니다.</p> <p>Pretrain-finetune 파이프라인은 pretrain 과정에서 각 downstream task들보다 훨씬 많은 데이터를 소모하는 것이 필수였고, 이는 data efficient한 LSTM에게는 별로 맞지 않는 일이었습니다. 얼마 지나지 않아 2018년 10월에 그 유명한 <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, BERT가 공개되었고 대중들도 이제는 진짜 “attention is all you need”임을 알게되죠. 단, “if you have data to fit them all”의 조건은 숨겨진 채로요. 데이터 자본, 컴퓨팅 자본의 맛을 본 기업들은 그 이후로 data efficient 모델에 대한 연구를 적극적으로 행하고 있지 않습니다. 엄청난 양의 데이터와 큰 트랜스포머로 pretrain한 모델을 적은 양의 다운스트림에 finetune하고는 data efficient하다고 말하는 기만적인 연구는 빼고요. 이게 2015년 겨울 CS231n이 나온 뒤로 벌어진 일들입니다.</p> <h3 id="성역-없는-침범">성역 없는 침범</h3> <p>BERT 이후 2019년은 NLP에 있어서 숙청의 해였습니다. 모든 방법론들이 몰살당하고 pretrained transformer가 그 자리를 차지했죠. 아주 정교한 언어학적 parse tree를 structural bias로 준 모델 조차 많은 데이터로 프리트레인 된 transformer 앞에서는 그냥 초라한 퍼포먼스밖에 낼 수 없었습니다. NLP가 정리되고, 2019년 말~2020년 초에 image에서도 Language Modeling과 같이 데이터만 주어지면 그 데이터로부터 스트럭쳐를 뽑아내어 슈퍼비전으로 사용하는 <em>Self-supervised Learning</em>을 적용하기 위한 연구가 활발히 이루어집니다. 2019년 11월 <a href="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</a> (MoCo), 2020년 2월 <a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a> (SimCLR) 등이 대표적이고 이 모델들은 ELMo가 제시한 pretrain-finetune 방법을 이미지에도 적용함을 목표로 합니다. 예를 들어서 MoCo의 경우 10억장의 페이스북 독점 데이터셋인 Instagram-1B을 레이블 없이 이미지만 사용하고 그 CNN weight를 그대로 R-CNN에 백본으로 사용하면 이미지넷을 레이블과 함께 학습한 CNN보다 Object Detection 등의 다운스트림 태스크에서 더 높은 퍼포먼스를 낼 수 있음을 보입니다.</p> <p>그럼 구조상으로도 convolution이 설 자리를 없애는게 대규모의 데이터를 가진 기업 입장에서는 당연한 수순일 겁니다. 2020년 5월 <a href="https://arxiv.org/abs/2005.12872">End-to-End Object Detection with Transformers</a>, DETR에서는 detection과 segmentation에서 R-CNN 구조보다 더 좋은 성능을 transformer가 낼 수 있음을 보이고, 2020년 10월 <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>, ViT에서는 드디어 pixel에서 feature에 이르는 CNN이 독점해왔던 인코더를 transformer가 대체하고 더 좋은 성능을 낼 수 있음을 보입니다. 2021년 1월 현재, 아직까지 이미지에는 GLUE와 같은 “대표적인” 벤치마크 셋이 없습니다. 하지만 곧 ViT 구조로 아주 큰 데이터셋에서 pretrain을 수행하고 (self-supervised 방식으로) 여기에 BERT처럼 헤드를 붙여서 (detection, segmentation 등) 모든 task를 압도하는 모델과, 그걸 수치적으로 보여줄 벤치마크 셋이 곧 나올 것은 불을 보듯 뻔한 일입니다.</p> <h3 id="exploit을-하기-위해서-배워야-할-것">Exploit을 하기 위해서 배워야 할 것</h3> <p>기업들이 아직도 데이터는 공개 안해도 그 <em>pretrained weight</em>는 공개해준다는 것은 아주 다행인 일입니다. 하지만 그 와중에도 그중에서 제일 좋고 큰 모델의 weight는 공개 안하는게 어느덧 일반적이 되어가고 있습니다. 물론 그 weight는 일반적인 가속기로는 돌리기 꽤 버겁지만요. 그리고 앞서 말했듯 지금의 SoTA 모델은 복잡하고 정교한 인간의 inductive bias가 녹여진 모델이 아니라 그냥 <strong>큰 트랜스포머</strong>를 해당 태스크에 맞게 finetune한 것입니다. 설령 지금 당장 목표로 하는 응용분야에서 개별 모델이 득세하고 있다고 한들, 제 생각으로는 2023년 이내에 트랜스포머로 대체될 가능성이 농후합니다.</p> <p>이를 모델의 다양성이 없어진다고 볼 수도 있지만, 사실 자주 인용되는 밋첼의 머신러닝 학습의 정의 ‘프로그램이 태스크 T에 대해서 경험 E를 사용해서 계량값 P로 계량한 성능을 올리는 것을 학습이라고 한다’에서 보듯 학습이라는 측면에서 모델의 구조는 그냥 부차적인 요소일 뿐이고, 어떠한 경우에도 그에 맞춰줄 수 있게 capacity만 넉넉하면 상관이 없습니다. 즉 중요한 건 태스크 T, 경험 (데이터) E, 계량값 (메트릭) P의 개선과 더 유효한 정의지, 모델은 그냥 transformer로 충분하다는 얘기입니다.</p> <p>그럼 우리가 알아야할것은 아래의 다섯 가지로 추려볼 수 있습니다.</p> <ol> <li>연산 가속기를 다루는 방법</li> <li>CS231n과 같은 강의: 위에서 대략적으로 짚은 2016~2020의 역사 이전이지만, 또 Andrew Ng이나 Murphy의 강의에서 다루는 것들보다는 이후인 내용에 대한 이해가 필요합니다.</li> <li>위에서 제가 짚어둔 논문들과 그들이 논문에서 언급하는 background</li> <li>내가 응용을 원하는 분야의 task 이름과 그 task를 학습하기 위한 objective function의 구조, 그리고 대표적인 academic dataset과 평가 metric</li> <li>빅테크 기업들이 내가 응용하고자 하는 분야에 transformer를 적용했는가? 그렇다면 그 모델의 weight는 공개되어있는가?</li> </ol> <p>위의 세가지는 모든 사람들이 공통적으로 학습해야 하는 부분이며, 아래 두가지는 각자의 세부 분야에 맞게 학습해야 하는 부분입니다. 1번, 연산 가속기를 다루는 방법에 대해서는 딱히 정답이 없습니다. 부딪혀가면서 익히거나 <a href="https://pytorch-lightning.readthedocs.io/en/latest/">Pytorch Lightning</a>과 같은 애드온이 engineering demands를 덜어주기를 바라는 수밖에요. 4번에 대해서는 대략적인 가이드를 아래에서 드리겠지만, <a href="https://paperswithcode.com/sota">papers with code</a>를 참조하면서 동시에 해당 분야의 survey 페이퍼를 읽고서 시작하는 것을 강력히 추천합니다. 또 5번의 경우 weight가 공개되어있지 않다면, pretraining부터 수행해야하는 문제점이 있는데, academic dataset이라도 이것을 기업이 아닌 곳에서 수행하기는 굉장히 어려우니 공개된 쪽으로 방향을 돌리기를 추천드립니다.</p> <h4 id="nlp에서-transfer-learning에-대하여">NLP에서 Transfer Learning에 대하여</h4> <p>앞서 말했듯이 예전에는 의미를 가지는 태스크 (예를들어 이미지넷 분류)에서 학습된 모델을 트랜스퍼하는 형태였다면, 지금은 거대한 데이터를 레이블링 없이 특정 의미 없는 태스크 (self-supervision)로 학습한 다음 트랜스퍼하는 모델들이 성능이 월등합니다. 위에 언급한 논문들에 더해서 구글의 트랜스퍼 러닝에 대한 추가적인 연구를 다룬 <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>, T5와 그 백그라운드를 읽어보시길 추천드립니다. 또한 Huggingface의 <a href="https://huggingface.co/transformers/model_doc/t5.html">pretrained weight</a>와 <a href="https://github.com/huggingface/transformers">라이브러리 코드</a>를 이해하시기를 추천드립니다.</p> <h4 id="graph-data에-대하여">Graph data에 대하여</h4> <p>Graph가 핫 토픽이기는 하죠, 하지만 막상 응용하기엔 그렇게 쉽지 않은 영역입니다. 우선 GNN은 transformer와 어느정도 <a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">동치 관계</a>임을 알고 가셨으면 좋겠습니다. Message passing이라던가 multi-hop등의 그래프 특정 용어 또한 트랜스포머에서 흔히 하는 연산과 별 다를바가 없죠. 만약 GNN을 응용하고자 한다면 그 커뮤니티의 용어들의 용례를 잘 알고서 내가 인코드하고자 하는 데이터가 그래프 스트럭쳐인지, 노드인지, 엣지인지 아니면 그들의 조합인지를 잘 파악해야 합니다. 이 <a href="https://arxiv.org/abs/1901.00596">서베이 논문</a>을 읽고서 시작하시기를 추천드립니다.</p> <h4 id="xai에-대하여">XAI에 대하여</h4> <p>XAI는 굉장히 넓은 영역을 커버합니다. 또한 통일된 의견또한 없습니다. 2017년 HCI연구실과 국보연의 산학연구에서 저와 연구실분들이 작성한 자료에서는 머신러닝 모델의 시각화 방법론을 “point-based”와 “network-based”로 나누었는데요 (Shixia Liu의 <a href="https://ieeexplore.ieee.org/abstract/document/7536654">논문</a>을 따라서), 그 이후에 시각화를 넘어서서 더 다양한 방법으로 모델을 분석할 수 있게 되었고 현재 이 모든 방법들의 통칭이 XAI입니다.</p> <p>Interpretability vs Explainability, 두 용어의 차이에 대해서 여러가지 의견들이 있고, 통일된 의견은 없습니다. 제 의견은 Interpretability는 causality와 거의 같은 의미라고 보는 의견입니다. Interpretability란 모델 inference의 어떤 부분이 output의 cause인지를 사람이 알아낼 수 있는 정도입니다. 반면 Explainability는 모델 inference의 output에 대한 설명을 사람이나 모델 스스로 만들어내는 것을 의미합니다. Interpretability가 모델의 값들을 있는 그대로 드러내서 cause의 신뢰도를 사람이 판단하도록 한다면 explainability는 생성된 explanation으로 “내가 본 이미지가 멋져서요”라고 제시하고, 사람이 그 설명에 만족한다면 explainable하다고 볼 수 있는 것입니다. XAI가 explain의 방식으로 causal interpretation을 제시할 수도 있으므로, 두 개념은 교집합을 형성하지만 어느 한쪽이 다른 한쪽의 부분집합이 아니라고 생각합니다. 이는 제 의견입니다. 현재 학계에서는 별 의미 구분 없이 explainable and interpretable 같은 식으로 뭉뚱그려서 서술하는게 대세입니다. 다만 <a href="https://arxiv.org/abs/2006.11371">Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey</a>, 이 서베이 논문에서는 저와 거의 비슷한 정의를 내렸습니다.</p> <p>저는 개인적으로 아직 시각화 기법 자체가 XAI에 딱 적용된 사례는 없는 것 같아서 아쉽습니다: 즉 instance-based XAI 기법인 Grad-CAM을 적용한다고 했을때 이 시각화를 rainbow heatmap (VIS 전공자인 우리는 딱히 좋지 않다고 알고있는) 으로 보여주는게 대세인데, 어떤 방식으로 보여줬을때 사람들이 더 causality를 잘 느끼는지에 대한 연구가 없습니다. 우리 연구실의 전공을 살려서 해보기에 좋은 주제가 아닐까요?</p> <h4 id="할당-문제에-대하여">할당 문제에 대하여</h4> <p>할당 문제에 대한 접근은 multi-agent reinforcement learning으로도 가능하지만, optimal transport (OT)가 훨씬 효과적이고 많이 쓰이는 방법입니다. Sinkhorn algorithm의 저자인 Marco Cuturi가 2018년에 작성한 텍스트북인 <a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a>의 초반 파트를 읽어보시면 아마 어떻게 활용할 수 있는지가 그려질 것 같습니다. OT의 경우 모든 연산이 미분가능하기 때문에 최근 딥러닝과 융합도 잘 되고 있으며, 할당을 원하는 피쳐가 뉴럴넷으로 구해진 경우 인코더까지 동시에 학습할 수 있을 것입니다.</p> <h4 id="차원-축소와-뉴럴넷으로-인코드한-피쳐의-시각화적-사용법에-대하여">차원 축소와 뉴럴넷으로 인코드한 피쳐의 시각화적 사용법에 대하여</h4> <p>차원 축소, 이름은 차원 축소이지만 여러가지 태스크를 동시에 내포하고 있습니다. 예를 들어 레즈넷으로 이미지를 분류할때 224x224x3의 이미지를 2048 벡터로 인코드 했으니 차원이 축소됐다고 말할 수도 있지만, 차원 축소를 말할때 이런식의 인코딩을 의미하지는 않죠. 그런데 2048 벡터 대신에 3차원 벡터로 인코드하면? 이미지 분류를 위한 피쳐는 semantically 가까운 이미지들끼리 서로 피쳐간 거리도 가깝게 인코드 되는 경향이 있으므로 이건 차원 축소의 의미에 부합합니다. 하지만 3차원 벡터를 feature로 사용하면 이미지 분류에 필요한 rank보다 적게 되므로 분류 태스크를 잘 학습할수가 없고 결국 feature도 의미가 없어지겠죠. 따라서 raw data를 차원축소하기 위해서는 그 데이터를 특정 피쳐로 인코드하는 인코더가 학습된 상태로 준비되어 있어야 하고 그 피쳐를 저차원으로 끌어당기는 embedding이 필요합니다.</p> <p>응용을 위해 다음 두 가지를 생각해주셨으면 합니다.</p> <ol> <li>어떤 인코더를 사용해서 데이터를 n-dimension의 피쳐로 인코드할것인가?</li> <li>그 피쳐를 그대로 사용할 수 있는 상황인가? 꼭 2, 3차원으로 줄여야 하는가?</li> </ol> <p>분류 태스크로 얻어지는 피쳐는 어느정도 가까울거라는 예측은 할 수 있지만 그 보장은 없습니다. 하지만 피쳐끼리의 거리를 보장하는 학습법이 있습니다. 바로 Deep Metric Learning (DML)이라는 방법입니다. DML은 클래스 레이블을 사용해서 인코더를 학습하지 않고 데이터간의 유사성을 사용해서 인코더를 학습합니다. 그리고 그 유사성을 우리가 슈퍼비전으로 줘야하고요. <strong>자기 자신과만 가깝다</strong>라는 식으로 슈퍼비전을 줘서 레이블 없이도 self-supervision을 형성할 수 있습니다. 다만 위에서 말했듯 이 경우 데이터가 적으면 학습이 불가능합니다.</p> <p>DML을 통해서 얻어진 feature는 classification feature보다 훨씬 예쁘게 가까운 애들은 가깝도록, 먼 애들은 멀도록 인코드 됩니다. 사용할 encoder는 자유입니다. 흔히 이미지에 대해서 연구가 이루어지기 때문에 CNN을 쓰지만, 그래프 구조를 인코드하고 싶다면 GNN을 사용하고, 텍스트를 인코드하고 싶다면 RNN을 사용하면 됩니다; 혹은 transformer가 있는 영역이면 transformer를 사용하면 됩니다.</p> <p>DML의 여러 논문과 구현 코드는 <a href="https://github.com/KevinMusgrave/pytorch-metric-learning">이 레포</a>를 참조하세요. 이 레포의 태스크는 대부분 visual embedding을 획득하는 것을 목표로 하지만, 언어나 다른 모달리티에서도 DML 방법은 흔히 <em>contrastive learning</em>라는 이라는 이름으로 찾아볼 수 있습니다. 예로, 문장 임베딩을 하는 논문인 <a href="https://arxiv.org/abs/1902.08564">Improving Multilingual Sentence Embedding using Bi-directional Dual Encoder with Additive Margin Softmax</a>에서는 DML에 대한 언급은 없지만, 얼굴 이미지 DML 논문인 <a href="https://arxiv.org/abs/1801.05599">Additive margin softmax for face verification</a>을 중요하게 인용하고 있습니다.</p> <p>또 피쳐를 바로 visual mark를 사용해서 visually encode하는게 정답이 아닐 수도 있습니다. 그 데이터 포인트가 가진 여러 정보중에 더 spatially meaningful한 정보가 있다면 (e.g., GPS) spatial encoded mark는 그 정보에 양보하고 뉴럴넷으로 얻은 피쳐는 다른 층위 (detail 등)에서 사용하는 것을 고려해보는 게 어떨까요?</p> <h4 id="시계열-데이터의-처리에-관해">시계열 데이터의 처리에 관해</h4> <p>시계열 데이터는 시퀀스 데이터의 일부입니다. 시계열이 가지는 특징으로는 단조증가 독립변수 t의 값이 연속적이라는 특징이 있고, 데이터마다 t의 간격이 equivalent하지 않다는 특징이 있습니다. 딥러닝 era (2012~)에서 이 irregularly-sampled time series를 바로 다루는 일은 극히 최근까지 불가능하다고 여겨졌고, 많은 경우 데이터를 interpolate해서 regularly-sampled data (예컨대, 자연어 문장) 을 처리하는 모델을 통해 처리했습니다. t가 개입된 데이터를 효과적으로 다루는 방법은 2018년에 와서야 논문화 되었고 그 방법은 t를 모델 안에 내재시키는 방법입니다. (아주 간단히 말하면 기존 뉴럴넷의 레이어 뎁스가 N 같은 자연수였다면, 그 뎁스를 시간으로 사용하는 방법입니다; 그것도 연속적으로요)</p> <p>현재 SoTA 모델들은 전부 이런 모델 = Neural ODEs (더 넓게는, Deep Implicit Layers)의 구조를 취하고 있으며 Vector Institute에서 이와 관련한 연구를 활발히 수행하고 있습니다. 대표적으로는 다음 논문들이 있습니다. <a href="https://arxiv.org/abs/1806.07366">Neural Ordinary Differential Equations</a>, <a href="https://arxiv.org/abs/2002.08071">Dissecting Neural ODEs</a>, <a href="https://arxiv.org/abs/1907.03907">Latent ODEs for Irregularly-Sampled Time Series</a>. Neural ODE를 사용한 인코더인 ODE-RNN의 코드는 <a href="https://github.com/YuliaRubanova/latent_ode">이 레포</a>를 참조해보세요.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>해당 섹션의 permutation invariance 관련 내용을 좀 더 상세하게 수정하였습니다. (2021-01-23) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>이 아래로 inductive bias를 잘못 서술한 점이 있어서 지금 올바르다고 생각하는 방향으로 다시 서술하였습니다. permutation (in)variance에 관한 내용은 전개에 있어서 불필요하다 판단되어서 삭제하였습니다. (2021-02-08) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[HCI 연구자를 위한 최신 ML 모델 활용 가이드]]></summary></entry><entry><title type="html">Tensor #2. Einstein Notation and Maps</title><link href="https://wonjae.kim/blog/2019/tensor_2_einstein_notation_and_maps/" rel="alternate" type="text/html" title="Tensor #2. Einstein Notation and Maps"/><published>2019-03-30T00:00:00+00:00</published><updated>2019-03-30T00:00:00+00:00</updated><id>https://wonjae.kim/blog/2019/tensor_2_einstein_notation_and_maps</id><content type="html" xml:base="https://wonjae.kim/blog/2019/tensor_2_einstein_notation_and_maps/"><![CDATA[<p>XylyXylyX의 <a href="https://www.youtube.com/watch?v=_pKxbNyjNe8&amp;list=PLRlVmXqzHjUQARA37r4Qw3SHPqVXgqO6c">What is Tensor</a> 강의를 정리한 노트입니다.</p> <h3 id="einstein-notation">Einstein Notation</h3> <p><a href="/blog/2019/tensor_1_elementary_vector_space/">지난 편</a>에서 벡터 스페이스 \(\newcommand{\V}{\mathcal{V}} \newcommand{\R}{\mathbb{R}} (\V, +, \R)\)와 차원에 대해서 설명했습니다. 차원을 곁들여서 이제 벡터 스페이스를 \(\newcommand{\d}{\mathbf{d}} \newcommand{\VBa}[1]{(\V, +, \R, #1)} \newcommand{\VBb}[1]{(\mathcal{W}, +, \R, #1)} \VBa{\d}\)라고 쓰겠습니다. \(\d\)는 벡터 스페이스의 차원을 의미합니다.</p> <p>\(\d = 4\)라고 생각하고, 4차원 실수 벡터 공간 \(\VBa{4}\)를 생각해봅시다<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. 그럼 \(m \in {\VBa{4}}\)은 기저 벡터<sub>basis vector</sub><sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> \(w, v, p, g\)와 적절한 스칼라들 \(a, b, c, d\)의 선형 결합 \(m = aw + bv + cp + dg\)로 나타낼 수 있을 것입니다.</p> <p>늘 이런 적당한 알파벳을 찾는 것은 귀찮을 뿐더러 헷갈리기까지 합니다. 따라서 인덱싱<sub>indexing</sub>을 통해서 이 과정을 단순화하고자 합니다. 하나의 벡터를 완전히 표현하는데는 언제나 기저와 적당한 스칼라들의 곱과 합이 필요하기 때문에, 이것을 잘 나타내주는 표현이 필요한데요. 그 표기법을 아인슈타인 표기<sub>Einstein notation</sub>이라고 하겠습니다.</p> <p>임의의 4차원 벡터 \(a\)는 아인슈타인 표기로는 \(\newcommand{\a}{A^{\mu}e_{\mu}} \a\)라고 표기하며, 여기서 \(\mu\)는 더미 인덱스<sub>dummy index</sub>로, 위 첨자와 아래 첨자에 동시에 등장한다면 반복하며 곱해서 더하라는 뜻을 가집니다. 다시 말해 4차원 벡터일 경우 \(\a = \sum_{i}A^ie_i = A^0e_0 + A^1e_1 + A^2e_2 + A^3e_3\)입니다.</p> <h3 id="map">Map</h3> <p>두 벡터 스페이스 \(\VBa{4}\)와 \(\VBb{10}\) 사이의 관계를 정의할 수 있을까요? 물론 가능하고, 그걸 우리는 맵<sub>map</sub>이라고 부를 것입니다. 맵은 굉장히 여러 군데에서 쓰여서 그 표기도 다양한데요. 예를 몇 개 들면, \(\VBa{4}\)에서 \(\VBb{10}\)로 가는 맵 \(\Lambda : \mathcal{V} \rightarrow \mathcal{W}\)를 아래와 같이 표현할 수 있습니다.</p> <ul> <li>operator form: \(\Lambda v \rightarrow w\)</li> <li>function form: \(\Lambda(v) \rightarrow w\)</li> <li>operator와 function 사이 어딘가: \(\langle\Lambda, v\rangle \rightarrow w\) <ul> <li>이 경우 \(\langle\Lambda,\;\rangle\)는 operator 처럼 작용하게 됩니다.</li> </ul> </li> </ul> <p>앞으로 모든 노트에서는 \(\newcommand{\map}[2]{\langle#1,#2\rangle} \map{\;}{\;}\)을 사용하게 될 텐데, 그 편리함과 이유는 차차 나오게 됩니다.</p> <p>그럼 두 벡터 스페이스 \(\VBa{4}\)와 \(\VBb{10}\) 사이의 관계를 정의해볼까요? \(\VBa{4}\)의 모든 원소를 \(\VBb{10}\)에 대응시키기 위해선, 기저들의 관계만 정의해주면 됩니다. 왜냐하면 스칼라 필드는 둘 다 공유하고 있기 때문이죠 (실수). \(\VBa{4}\)의 기저 벡터를 \(e_i\)라고 하고 \(\VBb{10}\)의 기저 벡터를 \(f_j\)라고 한다면. 아래와 같은 꼴로 그 관계를 정의할 수 있을 겁니다.</p> <ul> <li> \[\map{\Lambda}{e_0} = 3f_1 + 2f_4 + f_{10}\] </li> <li> \[\map{\Lambda}{e_1} = \pi f_3 + e^{17}f_0\] </li> <li> \[\map{\Lambda}{e_2} = f_2\] </li> <li> \[\map{\Lambda}{e_3} = f_3 + f_5 + f_7 + f_9\] </li> </ul> <p>위의 네 가지 맵핑이 임의로 작성된 것은 충분히 느껴지실 겁니다. 네, 아무렇게나 작성할 수 있습니다. \(f_j\)의 차수가 1승이기만 한다면요. 이와 같은 맵을 선형 변환<sub>linear map</sub>이라고 합니다. 선형 변환은 아래와 같은 특징을 가집니다. 당연히.. 선형성이죠.</p> <h4 id="선형성linearity">선형성<sub>Linearity</sub></h4> <ul> <li> \[\map{\Lambda}{v+p} = \map{\Lambda}{v} + \map{\Lambda}{p}\] </li> <li> \[\map{\Lambda}{av+bp} = a\map{\Lambda}{v} + b\map{\Lambda}{p}\] </li> </ul> <p><a href="/blog/2019/tensor_1_elementary_vector_space/">지난 편</a>에서 두 벡터 스페이스 \(\VBa{4}\)와 \(\VBb{4}\)의 \(+\)는 서로 다르다고 했고, 두 공간에서 각각 뽑은 벡터 사이의 \(+\)는 <strong>정의되어 있지 않았다</strong>고 했는데요. 위 식에서 \(+\)는 그 조건을 잘 만족합니다. 좌변의 \(+\)는 \(\mathcal{V}\)의 \(+\)고, 우변의 \(+\)는 \(\mathcal{W}\)의 \(+\)입니다.</p> <p>이를 임의의 벡터 \(a = \a\)에 대해서 적어보면, \(\map{\Lambda}{\a} = A^0\map{\Lambda}{e_0} + A^1\map{\Lambda}{e_1} + \ldots\)가 될 것입니다. 물론 \(\map{\Lambda}{e_i}\)는 정의된 후겠죠.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>한국어와 영어를 혼용해서 쓸 것입니다. 이해 부탁드립니다. 😅 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>기저 벡터란 벡터 공간을 생성<sub>span</sub>할 벡터들을 의미합니다, 그저 그 뿐입니다. 이 경우 4차원 공간이니 기저 벡터도 4개겠죠. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Tensor 시리즈 2편: 아인슈타인 표기법과 맵]]></summary></entry><entry><title type="html">Tensor #1. Elementary Vector Space</title><link href="https://wonjae.kim/blog/2019/tensor_1_elementary_vector_space/" rel="alternate" type="text/html" title="Tensor #1. Elementary Vector Space"/><published>2019-03-28T00:00:00+00:00</published><updated>2019-03-28T00:00:00+00:00</updated><id>https://wonjae.kim/blog/2019/tensor_1_elementary_vector_space</id><content type="html" xml:base="https://wonjae.kim/blog/2019/tensor_1_elementary_vector_space/"><![CDATA[<p>XylyXylyX의 <a href="https://www.youtube.com/watch?v=_pKxbNyjNe8&amp;list=PLRlVmXqzHjUQARA37r4Qw3SHPqVXgqO6c">What is Tensor</a> 강의를 정리한 노트입니다.</p> <h3 id="vector는-특수한-조건condition을-가진-집합입니다">Vector는 특수한 조건<sub>condition</sub>을 가진 집합입니다.</h3> <ol> <li>Addition<br/> Vector Space \(\newcommand{\V}{\mathcal{V}} \V\)에 대해서 \(\newcommand{\and}{\;\&amp;\;} x \in \V \and y \in \V \rightarrow (x + y) \in \V\)</li> <li>Scalar Multiplication<br/> \(\newcommand{\R}{\mathbb{R}} a \in \R, v \in \V \rightarrow ax \in \V\), 여기서 \(\R\)은 실수 체<sub>field</sub>이며, 다른 어떠한 필드도 가능합니다. (복소수, 사원수<sub>quaternion</sub>, 팔원수<sub>octonion</sub> 등..)</li> </ol> <p>이 정의에 필수적인 요소들을 모아서 이렇게 씁시다 : \(\newcommand{\VBa}{(\V, +, \R)} \VBa\)<br/> 각각 집합 \(\V\), Addition \(+\), Scalar Field (이 경우에는 실수체) \(\R\)을 의미합니다.</p> <p>여기서 추가적으로 주의해야 할 것은 \(\VBa\)의 \(+\)와 \(\newcommand{\VBb}{(\mathcal{W}, +, \R)} \VBb\)의 \(+\)가 다르다는 점입니다. 다시 말해서 \(w \in \VBa, q \in \VBa\)일 때 \(w + q\)의 \(+\)와 \(r \in \VBb, s \in \VBb\)일 때 \(r + s\)의 \(+\)가 같지 않다는 거죠. \(w + s\)와 같은 \(+\)는 <strong>정의되지 않았습니다.</strong></p> <h3 id="벡터는-아래와-같은-성질property을-가집니다">벡터는 아래와 같은 성질<sub>property</sub>을 가집니다.</h3> <ol> <li>Linearity<br/> \(x \in \V, y \in \V, a \in \R \rightarrow ax + ay = a(x+y) \in \V\)</li> <li>Opposite<br/> \(-1 \in \R, x \in \V \rightarrow -x \in \V \and x + (-x) = 0\)</li> </ol> <blockquote> <p>Q1. 벡터 스페이스의 종류는 그럼 어떻게 결정되나요?<br/> A1. Scalar Field의 종류에 의해 결정됩니다. \(\R\)이라면 실수 벡터, \(\mathbb{C}\)라면 복소수 벡터, 이런 식이죠.</p> </blockquote> <blockquote> <p>Q2. 만약 두 벡터 스페이스 \(\VBa\)와 \(\VBb\)가 있다면, 두 벡터 스페이스는 이름 말고 뭐가 다를 수 있나요?<br/> A2. 심볼릭하게는, 다른 점이 없습니다. 아마도 차원<sub>dimension</sub>이 있다면 그들을 구분지을 수 있겠네요.</p> </blockquote> <h3 id="차원이-뭘까요">차원이 뭘까요?</h3> <p>\(q \in \VBa\)를 만들 수 있는 벡터의 최소 집합<sub>minimal set</sub>의 크기를 차원이라고 합니다. \(q\)가 \(\VBa\)의 벡터 원소인 \(w, p, n, o \in \V\)와 스칼라 원소인 \(a, b, c, d \in \R\)의 결합 \(q = aw + bp + cn + do\)로 표현되어질 수 있고, 이것보다 더 적은 수의 벡터 원소로는 표현될 수 없다면, \(\VBa\)의 차원은 4차원이겠네요.</p> <p>만약 두 벡터 스페이스의 스칼라 필드의 종류가 같고, 차원이 같다면 두 벡터 스페이스는 <em>Isomorphic</em>하다고 합니다.</p> <h3 id="기초적인elementary-벡터-스페이스">기초적인<sub>elementary</sub> 벡터 스페이스</h3> <p>여기까지의 정의로 얻어진 벡터 스페이스를 <em>Elementary Vector Space</em>라고 하겠습니다. 우리가 기존에 벡터를 배울 때 같이 따라왔던 내적<sub>inner product</sub>, 데카르트 곱<sub>cartesian product</sub>, 노름<sub>norm</sub>등은 정의되어있지 않습니다. 이러한 성질들이 없어도, 벡터 스페이스는 벡터 스페이스입니다. 이러한 성질들을 추가로 가진 벡터 스페이스들은 <em>Advanced Vector Space</em>라고 하겠습니다. 이후에 Tensor Product를 설명하면서 데카르트 곱, Metric Tensor를 설명하면서 내적과 노름이 자연스럽게 정의되게 됩니다.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Tensor 시리즈 1편: 벡터 스페이스의 기본 정의와 성질]]></summary></entry></feed>