---
---

@inproceedings{jung2017chartsense,
  abbr         = {CHI},
  title        = {ChartSense: Interactive data extraction from chart images},
  author       = {Jung, Daekyoung and Kim, Wonjae and Song, Hyunjoo and Hwang, Jeong-in and Lee, Bongshin and Kim, Bohyoung and Seo, Jinwook},
  booktitle    = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  pages        = {6706--6717},
  year         = {2017},
  organization = {ACM},
  pdf          = {https://dl.acm.org/citation.cfm?id=3025957},
  abstract     = {Charts are commonly used to present data in digital documents such as web pages, research papers, or presentation slides. When the underlying data is not available, it is necessary to extract the data from a chart image to utilize the data for further analysis or improve the chart for more accurate perception. In this paper, we present ChartSense, an interactive chart data extraction system. ChartSense first determines the chart type of a given chart image using a deep learning based classifier, and then extracts underlying data from the chart image using semi-automatic, interactive extraction algorithms optimized for each chart type. To evaluate chart type classification accuracy, we compared ChartSense with ReVision, a system with the state-of-the-art chart type classifier. We found that ChartSense was more accurate than ReVision. In addition, to evaluate data extraction performance, we conducted a user study, comparing ChartSense with WebPlotDigitizer, one of the most effective chart data extraction tools among publicly accessible ones. Our results showed that ChartSense was better than WebPlotDigitizer in terms of task completion time, error rate, and subjective preference.}
}

@inproceedings{jo2017swifttuna,
  abbr         = {PacificVis},
  title        = {SwiftTuna: Responsive and incremental visual exploration of large-scale multidimensional data},
  author       = {Jo, Jaemin and Kim, Wonjae and Yoo, Seunghoon and Kim, Bohyoung and Seo, Jinwook},
  booktitle    = {2017 IEEE Pacific Visualization Symposium (PacificVis)},
  pages        = {131--140},
  year         = {2017},
  organization = {IEEE},
  pdf          = {https://ieeexplore.ieee.org/abstract/document/8031587},
  abstract     = {For interactive exploration of large-scale data, a preprocessing scheme (e.g., data cubes) has often been used to summarize the data and provide low-latency responses. However, such a scheme suffers from a prohibitively large amount of memory footprint as more dimensions are involved in querying, and a strong prerequisite that specific data structures have to be built from the data before querying. In this paper, we present SwiftTuna, a holistic system that streamlines the visual information seeking process on large-scale multidimensional data. SwiftTuna exploits an in-memory computing engine, Apache Spark, to achieve both scalability and performance without building precomputed data structures. We also present a novel interactive visualization technique, tailed charts, to facilitate large-scale multidimensional data exploration. To support responsive querying on large-scale data, SwiftTuna leverages an incremental processing approach, providing immediate low-fidelity responses (i.e., prompt responses) as well as delayed high-fidelity responses (i.e., incremental responses). Our performance evaluation demonstrates that SwiftTuna allows data exploration of a real-world dataset with four billion records while preserving the latency between incremental responses within a few seconds.}
}

@article{kim2018understanding,
  title    = {Understanding Visualization Idioms Through Deep Visualization},
  author   = {Kim, Wonjae},
  year     = {2018},
  journal  = {Seoul National University},
  pdf      = {http://s-space.snu.ac.kr/handle/10371/141564},
  abstract = {Visualization (vis) idioms define the way visual representations are created and manipulated. Well-known vis idioms include familiar charts such as bar charts and pie charts. Vis research and applications require thorough understanding of vis idioms. As a medium to understand vis idioms, I suggest a novel approach that employs deep visualization. Deep visualization is a collective name of methods for visualizing the characteristics of neurons in deep neural networks by generating their preferred stimuli (images). In this paper, I present two neural networks, one classifying the type of vis idioms, and one generating images of given idiom type. By applying deep visualization on the neural classifier through the neural generator, I examine how deep visualization can help vis researchers understand diverse aspects of vis idioms in novel ways and potentially derive unexplored idioms.}
}

@inproceedings{kim2019learning,
  abbr      = {NeurIPS},
  title     = {Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning},
  author    = {Kim, Wonjae and Lee, Yoonho},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {6019--6030},
  year      = {2019},
  arxiv     = {1905.11666},
  code      = {https://github.com/kakao/DAFT},
  abstract  = {Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention-based reasoning by modelling it as a continuous dynamical system using neural ordinary differential equations. As a proof of concept, we augment a state-of-the-art visual reasoning model with DAFT. Our experiments reveal that applying DAFT yields similar performance to the original model while using fewer reasoning steps, showing that it implicitly learns to skip unnecessary steps. We also propose a new metric, Total Length of Transition (TLT), which represents the effective reasoning step size by quantifying how much a given model's focus drifts while reasoning about a question. We show that adding DAFT results in lower TLT, demonstrating that our method indeed obeys the human prior towards shorter reasoning paths in addition to producing more interpretable attention maps. Our code is available at https://github.com/kakao/DAFT.}
}

@article{lee2019discrete,
  title    = {Discrete Infomax Codes for Meta-Learning},
  author   = {Lee, Yoonho and Kim, Wonjae and Choi, Seungjin},
  journal  = {arXiv preprint arXiv:1905.11656},
  year     = {2019},
  arxiv    = {1905.11656},
  abstract = {Learning compact discrete representations of data is a key task on its own or for facilitating subsequent processing of data. In this paper we present a model that produces Discrete InfoMax Codes (DIMCO); we learn a probabilistic encoder that yields k-way d-dimensional codes associated with input data. Our model's learning objective is to maximize the mutual information between codes and labels with a regularization, which enforces entries of a codeword to be as independent as possible. We show that the infomax principle also justifies previous loss functions (e.g., cross-entropy) as its special cases. Our analysis also shows that using shorter codes, as DIMCO does, reduces overfitting in the context of few-shot classification. Through experiments in various domains, we observe this implicit meta-regularization effect of DIMCO. Furthermore, we show that the codes learned by DIMCO are efficient in terms of both memory and retrieval time compared to previous methods.}
}

@inproceedings{park2020diversified,
  title        = {Diversified Mutual Learning for Deep Metric Learning},
  author       = {Park, Wonpyo and Kim, Wonjae and You, Kihyun and Cho, Minsu},
  booktitle    = {European Conference on Computer Vision},
  pages        = {709--725},
  year         = {2020},
  organization = {Springer},
  abbr         = {ECCV-W},
  arxiv        = {2009.04170},
  abstract     = {Mutual learning is an ensemble training strategy to improve generalization by transferring individual knowledge to each other while simultaneously training multiple models. In this work, we propose an effective mutual learning method for deep metric learning, called Diversified Mutual Metric Learning, which enhances embedding models with diversified mutual learning. We transfer relational knowledge for deep metric learning by leveraging three kinds of diversities in mutual learning: (1) model diversity from different initializations of models, (2) temporal diversity from different frequencies of parameter update, and (3) view diversity from different augmentations of inputs. Our method is particularly adequate for inductive transfer learning at the lack of large-scale data, where the embedding model is initialized with a pretrained model and then fine-tuned on a target dataset. Extensive experiments show that our method significantly improves individual models as well as their ensemble. Finally, the proposed method with a conventional triplet loss achieves the state-of-the-art performance of Recall@1 on standard datasets: 69.9 on CUB-200-2011 and 89.1 on CARS-196.}
}

@inproceedings{pmlr-v139-kim21k,
  abbr      = {ICML},
  title     = {ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author    = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {5583--5594},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  month     = {18--24 Jul},
  publisher = {PMLR},
  arxiv     = {2102.03334},
  code      = {https://github.com/dandelin/vilt},
  abstract  = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.}
}

@inproceedings{lee2021conditional,
  title        = {Conditional Generation of Periodic Signals with Fourier-Based Decoder},
  author       = {Lee, Jiyoung and Kim, Wonjae and Gwak, Daehoon and Choi, Edward},
  booktitle    = {Advances in Neural Information Processing Systems},
  booksubtitle = {Deep Generative Models and Downstream Applications Workshop},
  year         = {2021},
  arxiv        = {2110.12365},
  code         = {https://github.com/jiyounglee-0523/FourierDecoder},
  abbr         = {NeurIPS-W},
  abstract     = {Periodic signals play an important role in daily lives. Although conventional sequential models have shown remarkable success in various fields, they still come short in modeling periodicity; they either collapse, diverge or ignore details. In this paper, we introduce a novel framework inspired by Fourier series to generate periodic signals. We first decompose the given signals into multiple sines and cosines and then conditionally generate periodic signals with the output components. We have shown our model efficacy on three tasks: reconstruction, imputation and conditional generation. Our model outperforms baselines in all tasks and shows more stable and refined results.}
}


@inproceedings{song2021vidt,
  title     = {ViDT: An Efficient and Effective Fully Transformer-based Object Detector},
  author    = {Song, Hwanjun and Sun, Deqing and Chun, Sanghyuk and Jampani, Varun and Han, Dongyoon and Heo, Byeongho and Kim, Wonjae and Yang, Ming-Hsuan},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  arxiv     = {2110.03921},
  code      = {https://github.com/naver-ai/vidt},
  abbr      = {ICLR},
  abstract  = {Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models at https://github.com/naver-ai/vidt.}
}

@inproceedings{moon2022speeding,
  abbr      = {CHI},
  author    = {Moon, Hee-Seung and Do, Seungwon and Kim, Wonjae and Seo, Jiwon and Chang, Minsuk and Lee, Byungjoo},
  title     = {Speeding up Inference with User Simulators through Policy Modulation},
  year      = {2022},
  isbn      = {9781450391573},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3491102.3502023},
  doi       = {10.1145/3491102.3502023},
  abstract  = {The simulation of user behavior with deep reinforcement learning agents has shown some recent success. However, the inverse problem, that is, inferring the free parameters of the simulator from observed user behaviors, remains challenging to solve. This is because the optimization of the new action policy of the simulated agent, which is required whenever the model parameters change, is computationally impractical. In this study, we introduce a network modulation technique that can obtain a generalized policy that immediately adapts to the given model parameters. Further, we demonstrate that the proposed technique improves the efficiency of user simulator-based inference by eliminating the need to obtain an action policy for novel model parameters. We validated our approach using the latest user simulator for point-and-click behavior. Consequently, we succeeded in inferring the user’s cognitive parameters and intrinsic reward settings with less than 1/1000 computational power to those of existing methods.},
  booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  articleno = {38},
  numpages  = {21},
  keywords  = {point-and-click, inverse modeling, simulation model},
  location  = {New Orleans, LA, USA},
  series    = {CHI '22}
}

@article{chun2022eccv,
  abbr     = {ECCV},
  title    = {ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},
  author   = {Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk and Oh, Seong Joon},
  journal  = {In ECCV 2022, to appear},
  year     = {2022},
  arxiv    = {2204.03359},
  abstract = {Image-Test matching (ITM) is a common task for evaluating the quality of Vision and Language (VL) models. However, existing ITM benchmarks have a significant limitation. They have many missing correspondences, originating from the data construction process itself. For example, a caption is only matched with one image although the caption can be matched with other similar images, and vice versa. To correct the massive false negatives, we construct the Extended COCO Validation (ECCV) Caption dataset by supplying the missing associations with machine and human annotators. We employ five state-of-the-art ITM models with diverse properties for our annotation process. Our dataset provides x3.6 positive image-to-caption associations and x8.5 caption-to-image associations compared to the original MS-COCO. We also propose to use an informative ranking-based metric, rather than the popular Recall@K(R@K). We re-evaluate the existing 25 VL models on existing and proposed benchmarks. Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K, CxC R@1 are highly correlated with each other, while the rankings change when we shift to the ECCV mAP. Lastly, we delve into the effect of the bias introduced by the choice of machine annotator. Source code and dataset are available in https://github.com/naver-ai/eccv-caption.}
}