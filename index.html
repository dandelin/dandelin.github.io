<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Wonjae (Dan) Kim</title>
<meta name="description" content="Wonjae (Dan) Kim
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47120899-4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-47120899-4');
  </script>


  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


</head>

<body
  class="fixed-top-nav ">

  <!-- Header -->

  <header>

  <!-- Nav Bar -->
  <nav id="navbar"
    class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              About
              
              <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
              Publications
              
            </a>
          </li>
          
          
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/assets/pdf/cv.pdf">
              CV
            </a>
          </li>
          
          <div class="toggle-container">
            <a id="light-toggle">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </a>
          </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>

  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

  <header class="post-header">
    <h1 class="post-title">
      <span class="font-weight-bold">Wonjae</span>
      (Dan) Kim / 김<span class="font-weight-bold">원재</span>
    </h1>
    <p class="desc">ML/HCI Research Scientist</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
      <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic5.jpg">
      
      
      <div class="address">
        <br/>

      </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am a lead research scientist at <a href="https://www.twelvelabs.io">TwelveLabs</a>.
Prior to joining TwelveLabs, I worked as a research scientist at <a href="https://naver-career.gitbook.io/en/teams/clova-cic">Naver AI LAB</a> and <a href="https://www.kakaocorp.com/?lang=en">Kakao</a>.
I completed my <a href="http://hcil.snu.ac.kr/people/wonjae-kim">M.Sc.</a> and <a href="https://cse.snu.ac.kr/en">B.Sc.</a> in computer science and engineering at Seoul National University.</p>

<p>I am interested in the following research topics:</p>

<ul>
  <li>Multimodal Representation Learning</li>
  <li>Self-supervised Representation Learning</li>
  <li>Human-Computer Interaction &amp; Information Visualization</li>
</ul>

    </div>

    
    <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">NeurIPS</abbr>
    
    
  </div>

  <div id="kim2019learning" class="col-sm-8">
    
    <div class="title">Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning</div>
    <div class="author">
      
      
      
      

      

      

      

      
      
      
      <em>Wonjae Kim</em>,
      
      
      
      
      
      

      

      

      

      
      
      
      
      and Yoonho Lee
      
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>32nd Conference on Neural Information Processing Systems (NeurIPS 2019)</em>
      
      <!-- 
        2019
       -->
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a href="http://arxiv.org/abs/1905.11666" class="btn btn-sm z-depth-0" role="button"
        target="_blank">arXiv</a>
      
      
      <a href="https://proceedings.neurips.cc/paper/2019/hash/ae3539867aaeec609a4260c6feb725f4-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      <a href="/assets/pdf/papers/daft.pdf" class="btn btn-sm z-depth-0" role="button"
        target="_blank">PDF</a>
      
      
      
      
      
      
      <a href="https://github.com/kakao/DAFT" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention-based reasoning by modelling it as a continuous dynamical system using neural ordinary differential equations. As a proof of concept, we augment a state-of-the-art visual reasoning model with DAFT. Our experiments reveal that applying DAFT yields similar performance to the original model while using fewer reasoning steps, showing that it implicitly learns to skip unnecessary steps. We also propose a new metric, Total Length of Transition (TLT), which represents the effective reasoning step size by quantifying how much a given model’s focus drifts while reasoning about a question. We show that adding DAFT results in lower TLT, demonstrating that our method indeed obeys the human prior towards shorter reasoning paths in addition to producing more interpretable attention maps. Our code is available at https://github.com/kakao/DAFT.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">ICML Long talk</abbr>
    
    
  </div>

  <div id="pmlr-v139-kim21k" class="col-sm-8">
    
    <div class="title">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</div>
    <div class="author">
      
      
      
      

      

      

      

      
      
      
      <em>Wonjae Kim<sup>*</sup></em>,
      
      
      
      
      
      

      

      

      

      
      
      
      
      Bokyung Son<sup>*</sup>,
      
      
      
      
      
      
      

      

      

      

      
      
      
      
      and Ildoo Kim
      
      
      
      
      
      
      <br><sup>*</sup>Equal contribution
      
    </div>

    <div class="periodical">
      
      <em>38th International Conference on Machine Learnings (ICML 2021)</em>
      
      <!-- 
        2021
       -->
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a href="http://arxiv.org/abs/2102.03334" class="btn btn-sm z-depth-0" role="button"
        target="_blank">arXiv</a>
      
      
      <a href="https://proceedings.mlr.press/v139/kim21k.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
      
      
      
      <a href="/assets/pdf/papers/vilt.pdf" class="btn btn-sm z-depth-0" role="button"
        target="_blank">PDF</a>
      
      
      
      
      
      <a href="https://icml.cc/virtual/2021/oral/9492" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
      
      
      <a href="https://github.com/dandelin/vilt" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.</p>
    </div>
    
  </div>
</div></li>
<li><div class="row">
  <div class="col-sm-2 abbr">
    
    
    <abbr class="badge">ECCV Oral</abbr>
    
    
  </div>

  <div id="kim2024hype" class="col-sm-8">
    
    <div class="title">HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts</div>
    <div class="author">
      
      
      
      

      

      

      

      
      
      
      <em>Wonjae Kim</em>,
      
      
      
      
      
      

      

      

      

      
      
      
      
      Sanghyuk Chun,
      
      
      
      
      
      
      

      

      

      

      
      
      
      
      Taekyung Kim,
      
      
      
      
      
      
      

      

      

      

      
      
      
      
      Dongyoon Han,
      
      
      
      
      
      
      

      

      

      

      
      
      
      
      and Sangdoo Yun
      
      
      
      
      
      
    </div>

    <div class="periodical">
      
      <em>19th European Conference on Computer Vision (ECCV 2024)</em>
      
      <!-- 
        2024
       -->
    </div>
    

    <div class="links">
      
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      <a href="http://arxiv.org/abs/2404.17507" class="btn btn-sm z-depth-0" role="button"
        target="_blank">arXiv</a>
      
      
      
      
      <a href="/assets/pdf/papers/hype.pdf" class="btn btn-sm z-depth-0" role="button"
        target="_blank">PDF</a>
      
      
      
      
      
      
      <a href="https://github.com/naver-ai/hype" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
      
      
      
      
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In an era where the volume of data drives the effectiveness of self-supervised learning, the specificity and clarity of data semantics play a crucial role in model training. Addressing this, we introduce HYPerbolic Entailment filtering (HYPE), a novel methodology designed to meticulously extract modality-wise meaningful and well-aligned data from extensive, noisy image-text pair datasets. Our approach leverages hyperbolic embeddings and the concept of entailment cones to evaluate and filter out samples with meaningless or underspecified semantics, focusing on enhancing the specificity of each data sample. HYPE not only demonstrates a significant improvement in filtering efficiency but also sets a new state-of-the-art in the DataComp benchmark when combined with existing filtering techniques. This breakthrough showcases the potential of HYPE to refine the data selection process, thereby contributing to the development of more accurate and efficient self-supervised learning models. Additionally, the image specificity ϵi can be independently applied to induce an image-only dataset from an image-text or image-only data pool for training image-only self-supervised models and showed superior performance when compared to the dataset induced by CLIP score.</p>
    </div>
    
  </div>
</div></li></ol>
</div>

    

    
    <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">2025/04</th>
          <td>
            
              One CVPR-2025 <a href="https://syndata4cv.github.io">EVAL-FoMo 2 Workshop</a> paper: <a href="https://sites.google.com/view/eval-fomo-2-cvpr/home">Emergence of Text Readability in Vision Language Models</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">2025/02</th>
          <td>
            
              I’ve started a new chapter at <a href="https://twelvelabs.io/">TwelveLabs</a>!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">2025/01</th>
          <td>
            
              One ICLR-2025 paper to appear: <a href="https://arxiv.org/abs/2410.18857">Probabilistic Language-Image Pre-Training</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">2024/12</th>
          <td>
            
              One AAAI-2025 paper to appear: <a href="https://arxiv.org/abs/2412.18404">Extract Free Dense Misalignment from CLIP</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">2024/07</th>
          <td>
            
              One ECCV-2024 oral paper to appear: <a href="https://arxiv.org/abs/2404.17507">HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts</a>.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
    <div class="social">
      <span class="contact-icon text-center">
  <a href="mailto:%63%6F%6E%74%61%63%74@%77%6F%6E%6A%61%65.%6B%69%6D"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=UpZ41EwAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/dandelin" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/wjk" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://twitter.com/dandelin_kim" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

      <div class="contact-note"></div>
    </div>
    
  </article>

</div>
  </div>

  <!-- Footer -->

  
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Wonjae (Dan) Kim.
    
    
  </div>
</footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>





<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>