<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Wonjae (Dan) Kim </title> <meta name="author" content="Wonjae (Dan) Kim"> <meta name="description" content="Wonjae (Dan) Kim "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?130ac8e72b6f874fedd9df8c98658d44"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://wonjae.kim/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Wonjae</span> (Dan) Kim / 김<span class="font-weight-bold">원재</span> </h1> <p class="desc">Lead Research Scientist @ TwelveLabs</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic5-480.webp 480w,/assets/img/prof_pic5-800.webp 800w,/assets/img/prof_pic5-1400.webp 1400w," type="image/webp" sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic5.jpg?04996c3011cdc81c7259bd033b4e6397" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic5.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>I lead the Embedding &amp; Search team at <a href="https://www.twelvelabs.io" rel="external nofollow noopener" target="_blank">TwelveLabs</a>, where we build multimodal foundation models for video understanding. I’m the first author of <a href="https://github.com/dandelin/ViLT" rel="external nofollow noopener" target="_blank">ViLT</a>, one of the early works that shaped efficient vision-language architectures. Previously, I was a research scientist at <a href="https://naver-career.gitbook.io/en/teams/clova-cic" rel="external nofollow noopener" target="_blank">Naver AI LAB</a> and <a href="https://www.kakaocorp.com/?lang=en" rel="external nofollow noopener" target="_blank">Kakao</a>, and I hold an <a href="http://hcil.snu.ac.kr/people/wonjae-kim" rel="external nofollow noopener" target="_blank">M.Sc.</a> and <a href="https://cse.snu.ac.kr/en" rel="external nofollow noopener" target="_blank">B.Sc.</a> from Seoul National University.</p> <p>My current research focuses on:</p> <ul> <li>Multimodal Representation Learning (video, audio, text)</li> <li>Large-scale Embedding &amp; Search Systems</li> <li>User Behavior Modeling for Search</li> </ul> <div style="clear: both; padding-top: 1em;"></div> <hr> <p><strong>We’re Hiring!</strong> I’m building a research team at TwelveLabs where your models ship to thousands of customers within months. We’re tackling joint embedding spaces across modalities and containerized asset search—problems that go beyond simple retrieval to true semantic understanding of video structure. If you want to see your work create real-world impact at scale, <a href="https://calendar.app.google/uwAWtaSZsh8VGJNx7" rel="external nofollow noopener" target="_blank">grab a coffee chat</a> with me. I’m looking for scientists and engineers who are excited to push video-language AI from idea to production. <a href="https://jobs.ashbyhq.com/twelve-labs/38e8e1c9-bf91-449c-b64a-c3f481099801" rel="external nofollow noopener" target="_blank">Join us in Seoul →</a></p> </div> <h2>news</h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Dec 01, 2025</th> <td> TwelveLabs releases <a href="https://www.twelvelabs.io/blog/marengo-3-0" rel="external nofollow noopener" target="_blank">Marengo 3.0</a>, a new standard for foundation models that understand the world in all its complexity. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 15, 2025</th> <td> One ICCV-2025 paper to appear: <a href="https://github.com/jaeseokbyun/RTD" rel="external nofollow noopener" target="_blank">An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text Encoders for Composed Image Retrieval</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 01, 2025</th> <td> One CVPR-2025 <a href="https://syndata4cv.github.io" rel="external nofollow noopener" target="_blank">EVAL-FoMo 2 Workshop</a> paper: <a href="https://sites.google.com/view/eval-fomo-2-cvpr/home" rel="external nofollow noopener" target="_blank">Emergence of Text Readability in Vision Language Models</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 04, 2025</th> <td> I’ve started a new chapter at <a href="https://twelvelabs.io/" rel="external nofollow noopener" target="_blank">TwelveLabs</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 01, 2025</th> <td> One ICLR-2025 paper to appear: <a href="https://arxiv.org/abs/2410.18857" rel="external nofollow noopener" target="_blank">Probabilistic Language-Image Pre-Training</a>. </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">latest posts</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 11, 2025</th> <td> <a class="news-title" href="/blog/2025/The-gentle-singularity/">The Gentle Singularity</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 27, 2024</th> <td> <a class="news-title" href="/blog/2024/Deep-seek-interview/">DeepSeek: A More Extreme Story of Chinese Tech Idealism</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 02, 2021</th> <td> <a class="news-title" href="/blog/2021/Exploiting_Contemporary_ML/">Exploiting Contemporary ML</a> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV Oral</abbr> </div> <div id="kim2024hype" class="col-sm-8"> <div class="title">HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts</div> <div class="author"> <u>Wonjae Kim</u>, Sanghyuk Chun, Taekyung Kim, Dongyoon Han, and Sangdoo Yun </div> <div class="periodical"> <em>In 17th European Conference on Computer Vision (ECCV 2024)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.17507" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/papers/hype.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/naver-ai/hype" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In an era where the volume of data drives the effectiveness of self-supervised learning, the specificity and clarity of data semantics play a crucial role in model training. Addressing this, we introduce HYPerbolic Entailment filtering (HYPE), a novel methodology designed to meticulously extract modality-wise meaningful and well-aligned data from extensive, noisy image-text pair datasets. Our approach leverages hyperbolic embeddings and the concept of entailment cones to evaluate and filter out samples with meaningless or underspecified semantics, focusing on enhancing the specificity of each data sample. HYPE not only demonstrates a significant improvement in filtering efficiency but also sets a new state-of-the-art in the DataComp benchmark when combined with existing filtering techniques. This breakthrough showcases the potential of HYPE to refine the data selection process, thereby contributing to the development of more accurate and efficient self-supervised learning models. Additionally, the image specificity ϵi can be independently applied to induce an image-only dataset from an image-text or image-only data pool for training image-only self-supervised models and showed superior performance when compared to the dataset induced by CLIP score.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML Long talk</abbr> </div> <div id="pmlr-v139-kim21k" class="col-sm-8"> <div class="title">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</div> <div class="author"> <u>Wonjae* Kim</u>, Bokyung* Son, and Ildoo Kim </div> <div class="periodical"> <em>In 38th International Conference on Machine Learnings (ICML 2021)</em>, 18–24 jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2102.03334" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.mlr.press/v139/kim21k.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/papers/vilt.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://icml.cc/virtual/2021/oral/9492" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/dandelin/vilt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="kim2019learning" class="col-sm-8"> <div class="title">Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning</div> <div class="author"> <u>Wonjae Kim</u> and Yoonho Lee </div> <div class="periodical"> <em>In 32nd Conference on Neural Information Processing Systems (NeurIPS 2019)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1905.11666" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.neurips.cc/paper/2019/hash/ae3539867aaeec609a4260c6feb725f4-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/papers/daft.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/kakao/DAFT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention-based reasoning by modelling it as a continuous dynamical system using neural ordinary differential equations. As a proof of concept, we augment a state-of-the-art visual reasoning model with DAFT. Our experiments reveal that applying DAFT yields similar performance to the original model while using fewer reasoning steps, showing that it implicitly learns to skip unnecessary steps. We also propose a new metric, Total Length of Transition (TLT), which represents the effective reasoning step size by quantifying how much a given model’s focus drifts while reasoning about a question. We show that adding DAFT results in lower TLT, demonstrating that our method indeed obeys the human prior towards shorter reasoning paths in addition to producing more interpretable attention maps. Our code is available at https://github.com/kakao/DAFT.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="/assets/pdf/cv.pdf" title="CV" target="_blank"><i class="ai ai-cv"></i></a> <a href="mailto:%63%6F%6E%74%61%63%74@%77%6F%6E%6A%61%65.%6B%69%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/dandelin" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/wjk" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=UpZ41EwAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/dandelin_kim" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Wonjae (Dan) Kim. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47120899-4"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>